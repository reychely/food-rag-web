{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Food / Diet / Nutrition RAG (Multi‚ÄëSource)\n",
    "### TXT + PDF) ‚Üí Ingestion ‚Üí Chunking ‚Üí Index (Chroma) ‚Üí Query Improve ‚Üí Hybrid Retrieval ‚Üí Cross‚ÄëEncoder Rerank ‚Üí Self‚ÄëCheck ‚Üí Evaluation ‚Üí UI\n",
    ".\n",
    "**Supported source types**\n",
    "- **TXT** (`.txt`)\n",
    "- **PDF** (`.pdf`) ‚Äî extracted with PyMuPDF (fallback: pdfplumber)\n",
    "\n",
    "**Folder convention** (recommended)\n",
    "- `data/txt/` for TXT\n",
    "- `data/pdf/` for PDFs\n",
    "\n",
    "You can also put files directly under `data/` and the loader will still find them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## STEP 0 ‚Äî Setup (PyCharm-friendly)\n",
    "Ensures paths work even if the notebook is stored under `notebooks/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "cwd = Path.cwd()\n",
    "if cwd.name.lower() == \"notebooks\":\n",
    "    os.chdir(cwd.parent)\n",
    "\n",
    "print(\"CWD:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## STEP 1 ‚Äî Install & Imports\n",
    "Install dependencies (run once). The notebook has fallbacks when optional libs are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (+ optional install only-if-missing)\n",
    "# Works in Jupyter and is safe for a git repo: won't reinstall packages if already present.\n",
    "\n",
    "import sys, importlib.util, subprocess\n",
    "\n",
    "# --- list your packages here (pip name -> import name) ---\n",
    "REQUIRED = {\n",
    "    \"sentence-transformers\": \"sentence_transformers\",\n",
    "    \"rank-bm25\": \"rank_bm25\",\n",
    "    \"chromadb\": \"chromadb\",\n",
    "    \"pyspellchecker\": \"spellchecker\",\n",
    "    \"rapidfuzz\": \"rapidfuzz\",\n",
    "    \"ipywidgets\": \"ipywidgets\",\n",
    "    \"nltk\": \"nltk\",\n",
    "    \"pymupdf\": \"fitz\",          # PyMuPDF import is fitz\n",
    "    \"pdfplumber\": \"pdfplumber\",\n",
    "    \"odfpy\": \"odf\",\n",
    "}\n",
    "\n",
    "def is_installed(import_name: str) -> bool:\n",
    "    return importlib.util.find_spec(import_name) is not None\n",
    "\n",
    "missing = [pip_name for pip_name, imp in REQUIRED.items() if not is_installed(imp)]\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Missing packages:\", missing if missing else \"None \")\n",
    "\n",
    "# Set this to True only if you want the notebook to auto-install missing deps.\n",
    "AUTO_INSTALL = False\n",
    "\n",
    "if missing and AUTO_INSTALL:\n",
    "    print(\"\\nInstalling missing packages (same interpreter as notebook)...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", *missing])\n",
    "    print(\" Install done.\\n\")\n",
    "\n",
    "# ---- Now import everything (safe: will either work or clearly show what's missing) ----\n",
    "import re, json, math, time, hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Embeddings & reranker\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "except Exception:\n",
    "    SentenceTransformer = None\n",
    "    CrossEncoder = None\n",
    "\n",
    "# Keyword retrieval\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "except Exception:\n",
    "    BM25Okapi = None\n",
    "\n",
    "# Query improve utilities\n",
    "try:\n",
    "    from spellchecker import SpellChecker\n",
    "except Exception:\n",
    "    SpellChecker = None\n",
    "\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "except Exception:\n",
    "    fuzz = None\n",
    "\n",
    "# Vector DB persistence\n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "except Exception:\n",
    "    chromadb = None\n",
    "    SentenceTransformerEmbeddingFunction = None\n",
    "\n",
    "# UI\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, Markdown, clear_output\n",
    "except Exception:\n",
    "    widgets = None\n",
    "\n",
    "# PDF extraction\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except Exception:\n",
    "    fitz = None\n",
    "\n",
    "try:\n",
    "    import pdfplumber\n",
    "except Exception:\n",
    "    pdfplumber = None\n",
    "\n",
    "# ODF extraction\n",
    "try:\n",
    "    from odf.opendocument import load as odf_load\n",
    "    from odf import text as odf_text, teletype as odf_teletype\n",
    "except Exception:\n",
    "    odf_load = None\n",
    "    odf_text = None\n",
    "    odf_teletype = None\n",
    "\n",
    "print(\"\\n=== Availability ===\")\n",
    "print(\"SentenceTransformer:\", bool(SentenceTransformer))\n",
    "print(\"CrossEncoder:\", bool(CrossEncoder))\n",
    "print(\"BM25:\", bool(BM25Okapi))\n",
    "print(\"Chroma:\", bool(chromadb))\n",
    "print(\"ipywidgets:\", bool(widgets))\n",
    "print(\"PyMuPDF:\", bool(fitz))\n",
    "print(\"pdfplumber:\", bool(pdfplumber))\n",
    "print(\"odfpy:\", bool(odf_load))\n",
    "\n",
    "if missing and not AUTO_INSTALL:\n",
    "    print(\"\\n To install dependencies, either:\")\n",
    "    print(\"1) set AUTO_INSTALL=True and re-run this cell, OR\")\n",
    "    print(\"2) run in terminal (recommended):\")\n",
    "    print(\"   python -m pip install -U \" + \" \".join(missing))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## STEP 2 ‚Äî Data Sources (separated by source type)\n",
    "We keep files separate by source type (TXT/PDF/ODF) but also support searching under `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "TXT_DIR = DATA_DIR / \"txt\"\n",
    "PDF_DIR = DATA_DIR / \"pdf\"\n",
    "ODF_DIR = DATA_DIR / \"odf\"\n",
    "\n",
    "SUPPORTED = {\n",
    "    \"txt\": [\".txt\"],\n",
    "    \"pdf\": [\".pdf\"],\n",
    "    \"odf\": [\".odt\", \".ods\", \".odp\"],\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class DocFile:\n",
    "    source_type: str\n",
    "    path: str\n",
    "    name: str\n",
    "\n",
    "def discover_files() -> List[DocFile]:\n",
    "    out: List[DocFile] = []\n",
    "    roots = [\n",
    "        (\"txt\", TXT_DIR),\n",
    "        (\"pdf\", PDF_DIR),\n",
    "        (\"odf\", ODF_DIR),\n",
    "        (\"_data\", DATA_DIR),  # fallback: scan all under data\n",
    "    ]\n",
    "\n",
    "    seen = set()\n",
    "    for _, root in roots:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "\n",
    "        for fp in root.rglob(\"*\"):\n",
    "            if not fp.is_file():\n",
    "                continue\n",
    "\n",
    "            ext = fp.suffix.lower()\n",
    "            if ext in SUPPORTED[\"txt\"]:\n",
    "                typ = \"txt\"\n",
    "            elif ext in SUPPORTED[\"pdf\"]:\n",
    "                typ = \"pdf\"\n",
    "            elif ext in SUPPORTED[\"odf\"]:\n",
    "                typ = \"odf\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            key = str(fp.resolve())\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "\n",
    "            out.append(DocFile(source_type=typ, path=str(fp), name=fp.name))\n",
    "\n",
    "    return sorted(out, key=lambda x: (x.source_type, x.name))\n",
    "\n",
    "doc_files = discover_files()\n",
    "print(\"Total files:\", len(doc_files))\n",
    "for f in doc_files[:20]:\n",
    "    print(f\"- [{f.source_type}] {f.path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## STEP 3 ‚Äî Extraction (TXT / PDF / ODF)\n",
    "Each source type is extracted into raw text.\n",
    "\n",
    "- **TXT:** UTF‚Äë8 decode\n",
    "- **PDF:** PyMuPDF (best-effort), fallback to pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_txt(fp: Path) -> str:\n",
    "    return fp.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "def read_pdf_pymupdf(fp: Path) -> str:\n",
    "    # Best-effort extraction (no OCR). If your PDFs are scanned images, you need OCR (not included here).\n",
    "    if fitz is None:\n",
    "        raise RuntimeError(\"PyMuPDF not available\")\n",
    "    doc = fitz.open(fp)\n",
    "    pages = []\n",
    "    for page in doc:\n",
    "        pages.append(page.get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "def read_pdf_pdfplumber(fp: Path) -> str:\n",
    "    if pdfplumber is None:\n",
    "        raise RuntimeError(\"pdfplumber not available\")\n",
    "    pages = []\n",
    "    with pdfplumber.open(fp) as pdf:\n",
    "        for p in pdf.pages:\n",
    "            pages.append(p.extract_text() or \"\")\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "def read_odf(fp: Path) -> str:\n",
    "    if odf_load is None:\n",
    "        raise RuntimeError(\"odfpy not available\")\n",
    "    doc = odf_load(str(fp))\n",
    "    # Extract text from <text:p> elements (best-effort).\n",
    "    ps = doc.getElementsByType(odf_text.P)\n",
    "    out = []\n",
    "    for p in ps:\n",
    "        out.append(odf_teletype.extractText(p))\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def extract_text(doc: DocFile) -> str:\n",
    "    fp = Path(doc.path)\n",
    "    if doc.source_type == \"txt\":\n",
    "        return read_txt(fp)\n",
    "    if doc.source_type == \"pdf\":\n",
    "        # prefer PyMuPDF, fallback to pdfplumber\n",
    "        if fitz is not None:\n",
    "            return read_pdf_pymupdf(fp)\n",
    "        if pdfplumber is not None:\n",
    "            return read_pdf_pdfplumber(fp)\n",
    "        raise RuntimeError(\"No PDF extractor available (install pymupdf or pdfplumber).\")\n",
    "    if doc.source_type == \"odf\":\n",
    "        return read_odf(fp)\n",
    "    raise ValueError(\"Unknown source_type: \" + doc.source_type)\n",
    "\n",
    "# quick smoke test\n",
    "if doc_files:\n",
    "    t = extract_text(doc_files[0])\n",
    "    print(\"First file:\", doc_files[0].name, \"| type:\", doc_files[0].source_type)\n",
    "    print(\"Chars:\", len(t))\n",
    "    print(t[:300], \"...\")\n",
    "else:\n",
    "    print(\"No files found under data/. Add files under data/txt, data/pdf, data/odf.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## STEP 4 ‚Äî Cleaning + Metadata + Chunking\n",
    "We normalize text, detect rough structure (paragraphs/sentences), then create overlapping chunks.\n",
    "\n",
    "Metadata stored per chunk:\n",
    "- `source_type`, `source_file`, `doc_id`\n",
    "- `topic` (food/nutrition/unknown), `language`\n",
    "- optional: `reliability`, `date`, `section`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_language_light(text: str) -> str:\n",
    "    return \"he\" if re.search(r\"[\\u0590-\\u05FF]\", text) else \"en\"\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    s = re.sub(r\"[‚Äú‚Äù]\", '\"', s)\n",
    "    s = re.sub(r\"[‚Äò‚Äô]\", \"'\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def split_paragraphs(s: str) -> List[str]:\n",
    "    return [p.strip() for p in s.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "def split_sentences_best_effort(s: str) -> List[str]:\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+\", s.strip())\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def guess_topic(text: str, filename: str) -> str:\n",
    "    t = text.lower()\n",
    "    fn = filename.lower()\n",
    "    nutrition_terms = [\"vitamin\",\"calorie\",\"protein\",\"fiber\",\"sodium\",\"cholesterol\",\"diet\",\"nutrition\",\"omega\",\"macro\",\"micronutrient\"]\n",
    "    food_terms = [\"ingredients\",\"recipe\",\"bake\",\"cook\",\"boil\",\"meal\",\"serving\",\"mix\",\"stir\"]\n",
    "    if any(x in fn for x in [\"nutrition\",\"diet\",\"health\"]) or any(x in t for x in nutrition_terms):\n",
    "        return \"nutrition\"\n",
    "    if any(x in fn for x in [\"recipe\",\"cook\",\"meal\"]) or any(x in t for x in food_terms):\n",
    "        return \"food\"\n",
    "    return \"unknown\"\n",
    "\n",
    "CHUNK_MAX_CHARS = 1200\n",
    "CHUNK_OVERLAP_CHARS = 200\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    source_type: str\n",
    "    source_file: str\n",
    "    text: str\n",
    "    meta: Dict\n",
    "\n",
    "def stable_id(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:10]\n",
    "\n",
    "def recursive_chunk(paragraph: str, max_chars: int, overlap: int) -> List[str]:\n",
    "    if len(paragraph) <= max_chars:\n",
    "        return [paragraph]\n",
    "    sents = split_sentences_best_effort(paragraph)\n",
    "    chunks, cur = [], \"\"\n",
    "    for sent in sents:\n",
    "        if len(cur) + len(sent) + 1 <= max_chars:\n",
    "            cur = (cur + \" \" + sent).strip()\n",
    "        else:\n",
    "            if cur:\n",
    "                chunks.append(cur)\n",
    "            cur = sent\n",
    "    if cur:\n",
    "        chunks.append(cur)\n",
    "    if overlap > 0 and len(chunks) > 1:\n",
    "        out, prev = [], \"\"\n",
    "        for c in chunks:\n",
    "            out.append((prev[-overlap:] + \" \" + c).strip() if prev else c)\n",
    "            prev = c\n",
    "        return out\n",
    "    return chunks\n",
    "\n",
    "def ingest_all(doc_files: List[DocFile]) -> List[Chunk]:\n",
    "    out: List[Chunk] = []\n",
    "    for doc in doc_files:\n",
    "        raw = extract_text(doc)\n",
    "        cleaned = clean_text(raw)\n",
    "        doc_id = Path(doc.name).stem\n",
    "        lang = detect_language_light(cleaned)\n",
    "        topic = guess_topic(cleaned, doc.name)\n",
    "\n",
    "        paras = split_paragraphs(cleaned)\n",
    "        for pi, p in enumerate(paras):\n",
    "            pieces = recursive_chunk(p, CHUNK_MAX_CHARS, CHUNK_OVERLAP_CHARS)\n",
    "            for ci, piece in enumerate(pieces):\n",
    "                cid = f\"{doc_id}::{doc.source_type}::p{pi}::c{ci}::{stable_id(piece)}\"\n",
    "                out.append(Chunk(\n",
    "                    chunk_id=cid,\n",
    "                    doc_id=doc_id,\n",
    "                    source_type=doc.source_type,\n",
    "                    source_file=doc.path,\n",
    "                    text=piece,\n",
    "                    meta={\n",
    "                        \"source_type\": doc.source_type,\n",
    "                        \"source_file\": doc.path,\n",
    "                        \"topic\": topic,\n",
    "                        \"language\": lang,\n",
    "                        \"reliability\": \"unknown\",\n",
    "                        \"date\": None,\n",
    "                        \"section\": None\n",
    "                    }\n",
    "                ))\n",
    "    return out\n",
    "\n",
    "chunks = ingest_all(doc_files)\n",
    "print(\"Chunks:\", len(chunks))\n",
    "if chunks:\n",
    "    print(\"Example:\", chunks[0].chunk_id)\n",
    "    print(chunks[0].text[:220], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## STEP 5 ‚Äî Indexing (Chroma persistence + fallback)\n",
    "We index chunks in a persistent vector DB (Chroma). If Chroma is unavailable, we compute an in-memory embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHROMA_DIR = \"chroma_food_rag\"\n",
    "\n",
    "if SentenceTransformer is None:\n",
    "    raise RuntimeError(\"Install sentence-transformers to build embeddings.\")\n",
    "\n",
    "def chroma_safe_metadata(meta: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Chroma metadata values must be only: bool | int | float | str\n",
    "    \"\"\"\n",
    "    safe = {}\n",
    "    for k, v in meta.items():\n",
    "        if v is None:\n",
    "            continue  # drop Nones\n",
    "        if isinstance(v, (bool, int, float, str)):\n",
    "            safe[k] = v\n",
    "        else:\n",
    "            # convert anything else to string\n",
    "            safe[k] = str(v)\n",
    "    return safe\n",
    "emb_model = SentenceTransformer(EMB_MODEL_NAME) if chunks else None\n",
    "\n",
    "chroma_col = None\n",
    "if chromadb is not None and SentenceTransformerEmbeddingFunction is not None and chunks:\n",
    "    client = chromadb.PersistentClient(path=CHROMA_DIR)\n",
    "    emb_fn = SentenceTransformerEmbeddingFunction(model_name=EMB_MODEL_NAME)\n",
    "    chroma_col = client.get_or_create_collection(\n",
    "        name=\"food_rag_chunks_multisource\",\n",
    "        embedding_function=emb_fn,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "\n",
    "    existing = set()\n",
    "    try:\n",
    "        existing = set(chroma_col.get(include=[]).get(\"ids\", []))\n",
    "    except Exception:\n",
    "        existing = set()\n",
    "\n",
    "    new_ids, new_docs, new_metas = [], [], []\n",
    "for c in chunks:\n",
    "    if c.chunk_id not in existing:\n",
    "        new_ids.append(c.chunk_id)\n",
    "        new_docs.append(c.text)\n",
    "        new_metas.append(chroma_safe_metadata(c.meta))\n",
    "\n",
    "if new_ids:\n",
    "    chroma_col.add(ids=new_ids, documents=new_docs, metadatas=new_metas)\n",
    "    print(f\"Added {len(new_ids)} new chunks to Chroma\")\n",
    "else:\n",
    "    print(\"Chroma already up to date\")\n",
    "\n",
    "vecs = None\n",
    "if chroma_col is None and emb_model is not None and chunks:\n",
    "    vecs = emb_model.encode([c.text for c in chunks], normalize_embeddings=True, show_progress_bar=True).astype(np.float32)\n",
    "    print(\"vecs:\", vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma sync (make DB exactly match current chunks)\n",
    "\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "def chroma_sync_to_current(chroma_col, chunks, batch_size: int = 5000):\n",
    "    if chroma_col is None:\n",
    "        print(\"chroma_col is None\")\n",
    "        return\n",
    "\n",
    "    # current ids from Python ingestion\n",
    "    current_ids = [c.chunk_id for c in chunks]\n",
    "    current_set = set(current_ids)\n",
    "\n",
    "    # stored ids in Chroma\n",
    "    stored = chroma_col.get(include=[])  # ids only\n",
    "    stored_ids = stored.get(\"ids\", [])\n",
    "    stored_set = set(stored_ids)\n",
    "\n",
    "    to_delete = sorted(list(stored_set - current_set))\n",
    "    to_add_ids = []\n",
    "    to_add_docs = []\n",
    "    to_add_metas = []\n",
    "\n",
    "    # index chunks by id for quick lookup\n",
    "    by_id = {c.chunk_id: c for c in chunks}\n",
    "    missing = sorted(list(current_set - stored_set))\n",
    "    for cid in missing:\n",
    "        c = by_id[cid]\n",
    "        to_add_ids.append(c.chunk_id)\n",
    "        to_add_docs.append(c.text)\n",
    "        # IMPORTANT: Chroma metadata cannot contain None ‚Üí convert to strings or remove Nones\n",
    "        meta = {k: v for k, v in c.meta.items() if v is not None}\n",
    "        to_add_metas.append(meta)\n",
    "\n",
    "    print(\"Current chunks:\", len(current_ids))\n",
    "    print(\"Stored in Chroma:\", len(stored_ids))\n",
    "    print(\"Will delete:\", len(to_delete))\n",
    "    print(\"Will add:\", len(to_add_ids))\n",
    "\n",
    "    # delete extra ids\n",
    "    if to_delete:\n",
    "        for i in range(0, len(to_delete), batch_size):\n",
    "            chroma_col.delete(ids=to_delete[i:i+batch_size])\n",
    "        print(\"Deleted extras\")\n",
    "\n",
    "    # add missing\n",
    "    if to_add_ids:\n",
    "        for i in range(0, len(to_add_ids), batch_size):\n",
    "            chroma_col.add(\n",
    "                ids=to_add_ids[i:i+batch_size],\n",
    "                documents=to_add_docs[i:i+batch_size],\n",
    "                metadatas=to_add_metas[i:i+batch_size],\n",
    "            )\n",
    "        print(\"Added missing\")\n",
    "\n",
    "    # verify\n",
    "    new_count = len(chroma_col.get(include=[]).get(\"ids\", []))\n",
    "    print(\"Final Chroma count:\", new_count)\n",
    "    print(\"Should match current:\", len(current_ids))\n",
    "\n",
    "# RUN IT\n",
    "chroma_sync_to_current(chroma_col, chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify latest files: If Chunks in Python > Chunks in Chroma, then it‚Äôs not actually up to date (usually because chunk_ids changed).\n",
    "print(\"Chunks in Python:\", len(chunks))\n",
    "\n",
    "if chroma_col is not None:\n",
    "    count = len(chroma_col.get(include=[]).get(\"ids\", []))\n",
    "    print(\"Chunks in Chroma:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## STEP 6 ‚Äî Hybrid Retrieval (BM25 + embeddings) + Query Improvement\n",
    "We:\n",
    "1) create query variants\n",
    "2) retrieve using embeddings + BM25\n",
    "3) fuse/deduplicate results\n",
    "\n",
    "This step improves recall without exploding the query:\n",
    "1) Normalize (lowercase, units)\n",
    "2) Domain-safe spell correction (lightweight)\n",
    "3) Synonym expansion using a real vocabulary (MeSH Entry Terms)\n",
    "4) Produce 2‚Äì4 short query variants and fuse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Make sure vocab  exists (load MeSH cache if available) ---\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "SYN_CACHE = Path(\"vocab/mesh_synonyms_cache.json\")\n",
    "\n",
    "if \"syn_vocab\" not in globals():\n",
    "    if SYN_CACHE.exists():\n",
    "        syn_vocab = json.loads(SYN_CACHE.read_text(\"utf-8\"))\n",
    "        print(f\"syn_vocab loaded from cache: {len(syn_vocab):,} entries\")\n",
    "    else:\n",
    "        syn_vocab = {}   # fallback (no MeSH synonyms)\n",
    "        print(\"syn_vocab not found (no cache). Using empty synonyms.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 + Query Improve (spell + MeSH synonyms)\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# --- BM25 ---\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "bm25 = None\n",
    "if BM25Okapi is not None and chunks:\n",
    "    bm25 = BM25Okapi([tokenize(c.text) for c in chunks])\n",
    "    print(\"BM25 ready\")\n",
    "else:\n",
    "    print(\"BM25 not available\")\n",
    "\n",
    "# --- Query normalization ---\n",
    "PROTECTED_TERMS = {\n",
    "    \"bmi\",\"bp\",\"ldl\",\"hdl\",\"vit\",\"omega\",\"omega-3\",\"dha\",\"epa\",\n",
    "    \"sodium\",\"potassium\",\"calcium\",\"magnesium\",\"iron\",\"zinc\",\"protein\",\"fiber\",\n",
    "    \"carbs\",\"kcal\",\"mg\",\"g\",\"kg\",\"ml\",\"iu\"\n",
    "}\n",
    "\n",
    "def normalize_query(q: str) -> str:\n",
    "    q = q.strip().lower()\n",
    "    q = re.sub(r\"\\s+\", \" \", q)\n",
    "    # units\n",
    "    q = q.replace(\"kilograms\", \"kg\").replace(\"kilogram\", \"kg\")\n",
    "    q = q.replace(\"milligrams\", \"mg\").replace(\"milligram\", \"mg\")\n",
    "    q = q.replace(\"grams\", \"g\").replace(\"gram\", \"g\")\n",
    "    return q\n",
    "\n",
    "def meaningful_tokens(q: str) -> List[str]:\n",
    "    toks = [t for t in tokenize(q) if len(t) > 1]\n",
    "    return toks\n",
    "\n",
    "# --- Spell correction (safe) ---\n",
    "def spell_fix_domain_safe(q: str) -> str:\n",
    "    q = normalize_query(q)\n",
    "    if SpellChecker is None:\n",
    "        return q\n",
    "\n",
    "    sp = SpellChecker()\n",
    "    toks = q.split()\n",
    "    out = []\n",
    "\n",
    "    for t in toks:\n",
    "        # keep tokens with digits/punct (omega-3, 2g, etc.)\n",
    "        if re.search(r\"[^a-z]\", t):\n",
    "            out.append(t); continue\n",
    "        if len(t) <= 2 or t in PROTECTED_TERMS:\n",
    "            out.append(t); continue\n",
    "\n",
    "        corr = sp.correction(t) or t\n",
    "\n",
    "        # accept only if close enough (avoid meaning changes)\n",
    "        if fuzz is not None and fuzz.ratio(t, corr) < 80:\n",
    "            corr = t\n",
    "\n",
    "        out.append(corr)\n",
    "\n",
    "    return \" \".join(out)\n",
    "\n",
    "# --- MeSH matching: generate n-gram keys from query to look up in syn_vocab ---\n",
    "def candidate_keys_from_query(q: str, max_ngram: int = 3) -> List[str]:\n",
    "    q = normalize_query(q)\n",
    "    toks = q.split()\n",
    "    keys = []\n",
    "    for n in range(max_ngram, 0, -1):\n",
    "        for i in range(0, len(toks) - n + 1):\n",
    "            keys.append(\" \".join(toks[i:i+n]))\n",
    "    # unique, keep longest first\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for k in keys:\n",
    "        if k not in seen:\n",
    "            uniq.append(k)\n",
    "            seen.add(k)\n",
    "    return uniq\n",
    "\n",
    "# --- Filter bad variants---\n",
    "BAD_PATTERNS = [r\"\\bpressure,\\s*blood\\b\", r\"\\bhigh pressure,\\s*blood\\b\"]\n",
    "def is_good_variant(v: str) -> bool:\n",
    "    v = v.strip().lower()\n",
    "    if any(re.search(p, v) for p in BAD_PATTERNS):\n",
    "        return False\n",
    "    if len(v.split()) > 16:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# --- ONE expand_query function ---\n",
    "def expand_query(q: str, max_alts: int = 4, syns_per_match: int = 2) -> List[str]:\n",
    "    base = spell_fix_domain_safe(q)\n",
    "    alts = [base]\n",
    "\n",
    "    # 1) Replace matched phrases with MeSH synonyms\n",
    "    keys = candidate_keys_from_query(base, max_ngram=3)\n",
    "    for key in keys:\n",
    "        syns = syn_vocab.get(key, [])[:syns_per_match] if syn_vocab else []\n",
    "        for s in syns:\n",
    "            cand = normalize_query(base.replace(key, s))\n",
    "            if cand != base and is_good_variant(cand) and len(meaningful_tokens(cand)) >= 2:\n",
    "                alts.append(cand)\n",
    "        if len(alts) >= max_alts:\n",
    "            break\n",
    "\n",
    "    # 2) Acronym expansion (generic, not hypertension-specific)\n",
    "    if \"bp\" in base and \"blood pressure\" not in base and len(alts) < max_alts:\n",
    "        alts.append(base.replace(\"bp\", \"blood pressure\"))\n",
    "\n",
    "    # dedupe + cap\n",
    "    uniq, seen = [], set()\n",
    "    for a in alts:\n",
    "        a = normalize_query(a)\n",
    "        if a and a not in seen and is_good_variant(a):\n",
    "            uniq.append(a); seen.add(a)\n",
    "    return uniq[:max_alts]\n",
    "\n",
    "# --- Retrieval helpers ---\n",
    "def minmax_norm(vals: List[float]) -> List[float]:\n",
    "    if not vals:\n",
    "        return []\n",
    "    mn, mx = min(vals), max(vals)\n",
    "    if mx - mn < 1e-9:\n",
    "        return [1.0 for _ in vals]\n",
    "    return [(v - mn) / (mx - mn) for v in vals]\n",
    "\n",
    "def retrieve_chroma(q: str, top_k: int = 10, where: Optional[Dict]=None) -> List[Dict]:\n",
    "    if chroma_col is None:\n",
    "        return []\n",
    "    res = chroma_col.query(\n",
    "        query_texts=[q],\n",
    "        n_results=top_k,\n",
    "        where=where,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],  # <-- DO NOT include \"ids\"\n",
    "    )\n",
    "    out = []\n",
    "    for cid, doc, meta, dist in zip(res[\"ids\"][0], res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
    "        out.append({\"chunk_id\": cid, \"text\": doc, \"meta\": meta, \"score\": 1.0 - float(dist)})\n",
    "    return out\n",
    "\n",
    "def retrieve_embed_fallback(q: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "    if vecs is None or emb_model is None:\n",
    "        return []\n",
    "    qv = emb_model.encode([q], normalize_embeddings=True, show_progress_bar=False).astype(np.float32)[0]\n",
    "    sims = vecs @ qv\n",
    "    idx = np.argsort(-sims)[:top_k]\n",
    "    return [(int(i), float(sims[i])) for i in idx]\n",
    "\n",
    "def retrieve_bm25(q: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "    if bm25 is None:\n",
    "        return []\n",
    "    scores = bm25.get_scores(tokenize(q))\n",
    "    idx = np.argsort(-scores)[:top_k]\n",
    "    return [(int(i), float(scores[i])) for i in idx]\n",
    "\n",
    "by_id = {c.chunk_id: c for c in chunks}\n",
    "\n",
    "def fusion_retrieve(question: str, fused_topk: int = 25, per_query_k: int = 12, where: Optional[Dict]=None):\n",
    "    variants = expand_query(question)\n",
    "    fused: Dict[str, float] = {}\n",
    "\n",
    "    for q in variants:\n",
    "        # embeddings\n",
    "        if chroma_col is not None:\n",
    "            for r in retrieve_chroma(q, top_k=per_query_k, where=where):\n",
    "                fused[r[\"chunk_id\"]] = max(fused.get(r[\"chunk_id\"], 0.0), 0.6 * float(r[\"score\"]))\n",
    "        else:\n",
    "            emb = retrieve_embed_fallback(q, top_k=per_query_k)\n",
    "            emb_scores = minmax_norm([s for _, s in emb])\n",
    "            for (i, _), sc in zip(emb, emb_scores):\n",
    "                fused[chunks[i].chunk_id] = max(fused.get(chunks[i].chunk_id, 0.0), 0.6 * sc)\n",
    "\n",
    "        # bm25\n",
    "        bm = retrieve_bm25(q, top_k=per_query_k)\n",
    "        bm_scores = minmax_norm([s for _, s in bm])\n",
    "        for (i, _), sc in zip(bm, bm_scores):\n",
    "            fused[chunks[i].chunk_id] = max(fused.get(chunks[i].chunk_id, 0.0), 0.4 * sc)\n",
    "\n",
    "    ranked = sorted(fused.items(), key=lambda x: x[1], reverse=True)[:fused_topk]\n",
    "    results = []\n",
    "    for cid, sc in ranked:\n",
    "        c = by_id.get(cid)\n",
    "        if not c:\n",
    "            continue\n",
    "        results.append({\n",
    "            \"chunk_id\": c.chunk_id,\n",
    "            \"text\": c.text,\n",
    "            \"meta\": c.meta,\n",
    "            \"source_file\": c.source_file,\n",
    "            \"source_type\": c.source_type,\n",
    "            \"score\": float(sc),\n",
    "        })\n",
    "    return results, variants\n",
    "\n",
    "# Demo\n",
    "q_demo = \"foods for high blood pressure\"\n",
    "retrieved, variants = fusion_retrieve(q_demo, fused_topk=25)\n",
    "print(\"Variants:\", variants)\n",
    "print(\"Top result:\", retrieved[0][\"chunk_id\"] if retrieved else \"‚Äî\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## STEP 7 ‚Äî Cross‚ÄëEncoder Reranking\n",
    "Reranking usually gives the biggest precision boost by scoring (question, chunk_text) pairs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker = CrossEncoder(RERANK_MODEL) if CrossEncoder is not None else None\n",
    "\n",
    "def cross_encoder_rerank(question: str, retrieved: List[Dict], top_n: int = 10) -> List[Dict]:\n",
    "    if reranker is None:\n",
    "        return retrieved[:top_n]\n",
    "    pairs = [(question, r[\"text\"]) for r in retrieved]\n",
    "    scores = reranker.predict(pairs)\n",
    "    out = []\n",
    "    for r, s in zip(retrieved, scores):\n",
    "        r2 = dict(r)\n",
    "        r2[\"rerank_score\"] = float(s)\n",
    "        out.append(r2)\n",
    "    out.sort(key=lambda x: x.get(\"rerank_score\", -1e9), reverse=True)\n",
    "    return out[:top_n]\n",
    "\n",
    "retrieved, variants = fusion_retrieve(q_demo, fused_topk=40)   # more candidates\n",
    "reranked = cross_encoder_rerank(q_demo, retrieved, top_n=10)\n",
    "\n",
    "for r in reranked[:3]:\n",
    "    print(r[\"chunk_id\"], \"| type:\", r[\"source_type\"], \"| rerank:\", round(r.get(\"rerank_score\", r[\"score\"]), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## STEP 8 ‚Äî Augmentation (prompt with citations)\n",
    "We build a strict prompt:\n",
    "- answer only from context\n",
    "- if missing: \"I don't know\"\n",
    "- end with citations (chunk IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_context(chunks_list: List[Dict], max_chunks: int = 6, max_chars_each: int = 900) -> str:\n",
    "    ctx = []\n",
    "    for r in chunks_list[:max_chunks]:\n",
    "        ctx.append(f\"[{r['chunk_id']}] (type={r['source_type']}, source={Path(r['source_file']).name})\\n{r['text'][:max_chars_each]}\")\n",
    "    return \"\\n\\n\".join(ctx)\n",
    "\n",
    "def build_prompt(question: str, chunks_list: List[Dict]) -> str:\n",
    "    context = build_context(chunks_list)\n",
    "    return f\"\"\"You are a helpful assistant.\n",
    "Answer the question ONLY using the provided context.\n",
    "If the context does not contain the answer, say: \"I don't know from the provided documents.\"\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Rules:\n",
    "- Use ONLY context facts.\n",
    "- End with: Citations: [chunk_id1, chunk_id2, ...]\n",
    "\"\"\"\n",
    "\n",
    "prompt = build_prompt(q_demo, reranked)\n",
    "print(prompt[:800], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## STEP 9 ‚Äî Generation + Self‚ÄëCorrection (verifier)\n",
    "What this step does:\n",
    " 1) Generates a DRAFT answer from retrieved context using Ollama\n",
    " 2) Runs a VERIFIER pass that removes unsupported claims and enforces citations\n",
    " 3) If Ollama is not running, it DOES NOT crash ‚Äî it switches to retrieval-only mode\n",
    "\n",
    "### Requirements: - Ollama installed and running : ollama pull llama3.2:3b\n",
    "- A model pulled, e.g.:  ollama pull llama3.1:8b # or smaller if your laptop is weak: ollama pull llama3.2:3b\n",
    "- Python package:pip install ollama\n",
    "\n",
    "Notes:\n",
    "- The verifier is essential for RAG: it reduces unsupported claims.\n",
    "- If you have weak hardware, switch to a smaller model:    OLLAMA_MODEL = \"llama3.2:3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests, re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "OLLAMA_HOST = \"http://127.0.0.1:11434\"\n",
    "OLLAMA_MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def ensure_ollama_ready_http(model: str) -> bool:\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n",
    "        r.raise_for_status()\n",
    "        models = [m.get(\"name\") for m in r.json().get(\"models\", []) if m.get(\"name\")]\n",
    "        if model not in models:\n",
    "            print(f\"Model '{model}' not found.\")\n",
    "            print(f\"Run once in terminal:  ollama pull {model}\")\n",
    "            print(\"Available models (first 15):\", models[:15])\n",
    "            return False\n",
    "        print(\" Ollama reachable | Model available:\", model)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\" Ollama not reachable:\", e)\n",
    "        print(\"Install + run Ollama: https://ollama.com/download\")\n",
    "        return False\n",
    "\n",
    "def ollama_chat(prompt: str, temperature: float = 0.2) -> str:\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You strictly follow instructions and cite sources.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": float(temperature)},\n",
    "    }\n",
    "    r = requests.post(f\"{OLLAMA_HOST}/api/chat\", json=payload, timeout=180)\n",
    "    r.raise_for_status()\n",
    "    return (r.json().get(\"message\", {}).get(\"content\") or \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Context building ----\n",
    "def build_context(chunks_list: List[Dict], max_chunks: int = 6, max_chars_each: int = 900) -> str:\n",
    "    ctx = []\n",
    "    for r in chunks_list[:max_chunks]:\n",
    "        src = Path(r.get(\"source_file\", \"\")).name if r.get(\"source_file\") else \"unknown\"\n",
    "        ctx.append(\n",
    "            f\"[{r['chunk_id']}] (type={r.get('source_type','?')}, source={src})\\n\"\n",
    "            f\"{(r.get('text','') or '')[:max_chars_each]}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(ctx)\n",
    "\n",
    "def print_top_chunks(chunks_list: List[Dict], n: int = 3, chars: int = 220):\n",
    "    print(\"\\n--- TOP CHUNKS (preview) ---\")\n",
    "    for r in chunks_list[:n]:\n",
    "        txt = (r.get(\"text\") or \"\").replace(\"\\n\", \" \")\n",
    "        print(f\"* {r.get('chunk_id')} | score={r.get('rerank_score', r.get('score', 0)):.3f}\")\n",
    "        print(\" \", txt[:chars], \"...\\n\")\n",
    "\n",
    "# ---- Prompt (forces citations per bullet) ----\n",
    "def build_prompt(question: str, chunks_list: List[Dict]) -> str:\n",
    "    context = build_context(chunks_list, max_chunks=6, max_chars_each=1100)\n",
    "    allowed_ids = [r[\"chunk_id\"] for r in chunks_list[:6]]\n",
    "\n",
    "    return f\"\"\"You are a careful nutrition assistant in a RAG system.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context (ONLY source of truth):\n",
    "{context}\n",
    "\n",
    "STRICT RULES:\n",
    "- Use ONLY facts explicitly stated in the context.\n",
    "- If the context does not contain the answer, say exactly:\n",
    "I don't know from the provided documents.\n",
    "- If you write bullets, EACH bullet MUST end with ONE citation like: [FULL_CHUNK_ID]\n",
    "- You may cite ONLY these chunk IDs:\n",
    "{allowed_ids}\n",
    "- Final line must be: Citations: [id1, id2, ...]\n",
    "\n",
    "Output format:\n",
    "1) 1 short answer sentence\n",
    "2) 0‚Äì5 bullets (only if supported)\n",
    "3) Final line: Citations: [...]\n",
    "\"\"\"\n",
    "\n",
    "# ---- Draft generation ----\n",
    "def generate_answer(question: str, chunks_list: List[Dict]) -> str:\n",
    "    if not chunks_list:\n",
    "        return \"I don't know from the provided documents.\\nCitations: []\"\n",
    "    return ollama_chat(build_prompt(question, chunks_list), temperature=0.2)\n",
    "\n",
    "# ---- Verifier (keeps only supported + cited bullets) ----\n",
    "def build_verifier_prompt(question: str, draft: str, chunks_list: List[Dict]) -> str:\n",
    "    context = build_context(chunks_list, max_chunks=8, max_chars_each=1100)\n",
    "    allowed = [r[\"chunk_id\"] for r in chunks_list[:8]]\n",
    "\n",
    "    return f\"\"\"You are a verifier for a RAG system.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Draft answer:\n",
    "{draft}\n",
    "\n",
    "Rules:\n",
    "1) Remove ANY claim not explicitly supported by the context.\n",
    "2) Remove any bullet that does not end with: [FULL_CHUNK_ID]\n",
    "3) Only allow citations from this list:\n",
    "{allowed}\n",
    "4) If not enough info, output exactly:\n",
    "I don't know from the provided documents.\n",
    "Citations: []\n",
    "\n",
    "Return ONLY the final corrected answer (must end with Citations: [...]).\n",
    "\"\"\"\n",
    "\n",
    "def verifier_pass(question: str, draft: str, chunks_list: List[Dict]) -> str:\n",
    "    if not chunks_list:\n",
    "        return \"I don't know from the provided documents.\\nCitations: []\"\n",
    "    return ollama_chat(build_verifier_prompt(question, draft, chunks_list), temperature=0.0)\n",
    "\n",
    "# ---- End-to-end Step 9 ----\n",
    "def rag_answer(\n",
    "    question: str,\n",
    "    fused_topk: int = 60,\n",
    "    per_query_k: int = 25,\n",
    "    rerank_top_n: int = 10,\n",
    "    verify: bool = True,\n",
    "    where: Optional[Dict] = None,\n",
    "    debug: bool = True,\n",
    "):\n",
    "    if not ensure_ollama_ready_http(OLLAMA_MODEL):\n",
    "        raise RuntimeError(\"Ollama/model not ready. See messages above.\")\n",
    "\n",
    "    retrieved, variants = fusion_retrieve(question, fused_topk=fused_topk, per_query_k=per_query_k, where=where)\n",
    "\n",
    "    try:\n",
    "        reranked = cross_encoder_rerank(question, retrieved, top_n=rerank_top_n)\n",
    "    except Exception:\n",
    "        reranked = retrieved[:rerank_top_n]\n",
    "\n",
    "    if debug:\n",
    "        print(\"Variants:\", variants)\n",
    "        print_top_chunks(reranked, n=3)\n",
    "\n",
    "    draft = generate_answer(question, reranked)\n",
    "    final = verifier_pass(question, draft, reranked) if verify else draft\n",
    "\n",
    "    print(\"\\n--- DRAFT ---\\n\", draft)\n",
    "    print(\"\\n--- FINAL ---\\n\", final)\n",
    "\n",
    "    return {\"question\": question, \"variants\": variants, \"top_chunks\": [r[\"chunk_id\"] for r in reranked], \"draft\": draft, \"final\": final}\n",
    "\n",
    "# Demo:\n",
    "# _ = rag_answer(\"foods for high blood pressure\", verify=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_demo = \"foods for high blood pressure\"\n",
    "\n",
    "retrieved, variants = fusion_retrieve(q_demo, fused_topk=60, per_query_k=25)\n",
    "reranked = cross_encoder_rerank(q_demo, retrieved, top_n=10)\n",
    "\n",
    "draft = ollama_chat(build_prompt(q_demo, reranked), temperature=0.2)\n",
    "final = verifier_pass(q_demo, draft, reranked)\n",
    "\n",
    "print(\"Variants:\", variants)\n",
    "print(\"\\n--- FINAL ---\\n\", final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Demo (optional) ----\n",
    "q_demo = \"benefits of fiber\"\n",
    "_ = rag_answer(q_demo, verify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Build a real synonym vocabulary from MeSH (one-time) + cache to JSON\n",
    "# Uses lxml(recover=True) so big XML parsing won't crash.\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# If this import fails, install into the SAME env as your Jupyter kernel:\n",
    "# In terminal:  python -m pip install lxml\n",
    "from lxml import etree\n",
    "\n",
    "MESH_XML = Path(\"vocab/mesh_desc.xml\")\n",
    "CACHE = Path(\"vocab/mesh_synonyms_cache.json\")\n",
    "CACHE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def norm(t: str) -> str:\n",
    "    return \" \".join((t or \"\").lower().split())\n",
    "\n",
    "def build_mesh_synonyms_cache(mesh_path: Path, cache_path: Path, max_terms_per_head: int = 25) -> dict:\n",
    "    if not mesh_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing MeSH XML at: {mesh_path.resolve()}\")\n",
    "\n",
    "    print(\"‚è≥ Parsing MeSH XML (this can take a few minutes once)...\")\n",
    "    parser = etree.XMLParser(recover=True, huge_tree=True)\n",
    "    root = etree.parse(str(mesh_path), parser).getroot()\n",
    "\n",
    "    syn_map = defaultdict(set)\n",
    "    n_records = 0\n",
    "\n",
    "    for rec in root.findall(\".//DescriptorRecord\"):\n",
    "        n_records += 1\n",
    "        head_el = rec.find(\"./DescriptorName/String\")\n",
    "        if head_el is None:\n",
    "            continue\n",
    "        head = norm(head_el.text)\n",
    "        if not head:\n",
    "            continue\n",
    "\n",
    "        terms = []\n",
    "        for term_el in rec.findall(\".//ConceptList/Concept/TermList/Term/String\"):\n",
    "            s = norm(term_el.text)\n",
    "            if s and s != head:\n",
    "                terms.append(s)\n",
    "\n",
    "        for s in terms[:max_terms_per_head]:\n",
    "            syn_map[head].add(s)\n",
    "            syn_map[s].add(head)\n",
    "\n",
    "        if n_records % 5000 == 0:\n",
    "            print(f\"  processed {n_records} records...\")\n",
    "\n",
    "    syn_vocab = {k: sorted(v) for k, v in syn_map.items()}\n",
    "\n",
    "    cache_path.write_text(json.dumps(syn_vocab, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    print(\"Cached to:\", cache_path.resolve())\n",
    "    return syn_vocab\n",
    "\n",
    "# Load cache if exists, otherwise build it\n",
    "if CACHE.exists():\n",
    "    syn_vocab = json.loads(CACHE.read_text(\"utf-8\"))\n",
    "    print(\"Loaded cached MeSH synonyms:\", len(syn_vocab))\n",
    "else:\n",
    "    syn_vocab = build_mesh_synonyms_cache(MESH_XML, CACHE, max_terms_per_head=25)\n",
    "    print(\"Built MeSH synonyms:\", len(syn_vocab))\n",
    "\n",
    "print(\"Example hypertension:\", syn_vocab.get(\"hypertension\", [])[:10])\n",
    "print(\"Example vitamin c:\", syn_vocab.get(\"vitamin c\", [])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## STEP 10 ‚Äî UI\n",
    "This creates a proper web interface reachable at http://127.0.0.1:7860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Dict, List\n",
    "import traceback\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# ---------- 0. IMAGE LOADER ----------\n",
    "def load_local_gif():\n",
    "    # Update this path if needed\n",
    "    paths_to_check = [\n",
    "        r\"C:\\Users\\reychel\\Documents\\GitHub\\food-rag-web\\gif\\giphy.gif\",\n",
    "        \"gif/giphy.gif\",\n",
    "        \"giphy.gif\"\n",
    "    ]\n",
    "    for p in paths_to_check:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                with open(p, \"rb\") as f:\n",
    "                    data = base64.b64encode(f.read()).decode('utf-8')\n",
    "                    return f\"data:image/gif;base64,{data}\"\n",
    "            except: pass\n",
    "    return None\n",
    "\n",
    "GIF_SOURCE = load_local_gif()\n",
    "\n",
    "# ---------- 1. CSS  ----------\n",
    "green_mist_css = \"\"\"\n",
    "/* APP BACKGROUND */\n",
    "body, .gradio-container {\n",
    "    background: radial-gradient(circle at 50% 0%, #064e3b 0%, #020617 60%) !important;\n",
    "    color: #ffffff !important;\n",
    "    font-family: 'Inter', sans-serif !important;\n",
    "}\n",
    "\n",
    "/* üü¢ TEXTBOXES & INPUTS (Force White) */\n",
    "textarea, input {\n",
    "    background-color: #020617 !important;\n",
    "    border: 1px solid #1e293b !important;\n",
    "    color: #ffffff !important;\n",
    "    font-weight: 500 !important;\n",
    "    border-radius: 10px !important;\n",
    "    opacity: 1 !important;\n",
    "}\n",
    "\n",
    "/* Force Read-Only to be WHITE */\n",
    "textarea[readonly], textarea:disabled {\n",
    "    color: #ffffff !important;\n",
    "    -webkit-text-fill-color: #ffffff !important;\n",
    "    background-color: #0f172a !important;\n",
    "    opacity: 1 !important;\n",
    "    border-color: #334155 !important;\n",
    "}\n",
    "\n",
    "/*  HIDE ALL INDIVIDUAL LOADING ANIMATIONS  */\n",
    ".loading { display: none !important; }\n",
    ".meta-text-container { display: none !important; }\n",
    ".pending { border-color: transparent !important; }\n",
    ".generating { border-color: transparent !important; }\n",
    "\n",
    "/*  HEADERS (QUERY, SETTINGS -> NEON GREEN) */\n",
    ".prose h1, .prose h2, .prose h3 {\n",
    "    color: #4ade80 !important;\n",
    "    font-weight: 800 !important;\n",
    "    letter-spacing: 1px;\n",
    "    margin-bottom: 5px;\n",
    "    margin-top: 0px;\n",
    "    opacity: 1 !important;\n",
    "    text-transform: uppercase;\n",
    "    line-height: 1.5;\n",
    "}\n",
    "\n",
    "/* MAIN TITLE */\n",
    ".header-title {\n",
    "    font-size: 3.5rem;\n",
    "    font-weight: 800;\n",
    "    background: linear-gradient(180deg, #ffffff 0%, #4ade80 100%);\n",
    "    -webkit-background-clip: text;\n",
    "    -webkit-text-fill-color: transparent;\n",
    "    text-shadow: 0 0 40px rgba(74, 222, 128, 0.4);\n",
    "    text-align: center;\n",
    "}\n",
    ".header-subtitle {\n",
    "    color: #86efac; text-align: center; font-size: 1.1rem; letter-spacing: 2px;\n",
    "    text-transform: uppercase; margin-bottom: 30px; opacity: 0.9;\n",
    "}\n",
    "\n",
    "/* LABELS */\n",
    ".block-label, label span {\n",
    "    color: #4ade80 !important;\n",
    "    font-weight: bold !important;\n",
    "    font-size: 0.9rem !important;\n",
    "    text-transform: uppercase;\n",
    "}\n",
    "\n",
    "/* üü¢ STATUS BAR CONTAINER */\n",
    ".status-container {\n",
    "    padding: 12px 20px;\n",
    "    border-radius: 12px;\n",
    "    margin-bottom: 20px;\n",
    "    font-family: 'Courier New', monospace; font-weight: 700;\n",
    "    display: flex; align-items: center; justify-content: space-between;\n",
    "    background: rgba(15, 23, 42, 0.95); /* More opaque to fix grey issue */\n",
    "    backdrop-filter: blur(5px);\n",
    "    margin-top: 0px;\n",
    "    height: 50px;\n",
    "}\n",
    "\n",
    "/* üü¢ NUCLEAR WHITE TEXT FIX FOR STATUS BAR */\n",
    ".status-container span, .status-container div {\n",
    "    color: #ffffff !important;\n",
    "    -webkit-text-fill-color: #ffffff !important;\n",
    "    opacity: 1 !important;\n",
    "}\n",
    "\n",
    "/* Status Phases */\n",
    ".phase-search { border: 1px solid #3b82f6; box-shadow: 0 0 15px rgba(59, 130, 246, 0.2); }\n",
    ".phase-rerank { border: 1px solid #f59e0b; box-shadow: 0 0 15px rgba(245, 158, 11, 0.2); }\n",
    ".phase-gen    { border: 1px solid #a855f7; box-shadow: 0 0 15px rgba(168, 85, 247, 0.2); }\n",
    ".phase-done   { border: 1px solid #22c55e; box-shadow: 0 0 20px rgba(34, 197, 94, 0.3); }\n",
    "\n",
    ".smart-spinner {\n",
    "    width: 20px; height: 20px; border: 3px solid #ffffff;\n",
    "    border-top-color: transparent; border-radius: 50%;\n",
    "    animation: spin 0.8s linear infinite;\n",
    "}\n",
    "@keyframes spin { 100% { transform: rotate(360deg); } }\n",
    "\n",
    "/* EVIDENCE CARDS */\n",
    ".evidence-card {\n",
    "    background: #111827; border: 1px solid #1f2937;\n",
    "    border-radius: 8px; padding: 12px; margin-bottom: 10px;\n",
    "}\n",
    ".evidence-card:hover { border-color: #4ade80; }\n",
    "\n",
    "/* BUTTONS */\n",
    "#search-btn {\n",
    "    background: linear-gradient(90deg, #22c55e 0%, #16a34a 100%);\n",
    "    border: 1px solid #4ade80; color: white; font-weight: 800; font-size: 1.1rem;\n",
    "    height: 50px; border-radius: 10px;\n",
    "}\n",
    "#search-btn:hover { box-shadow: 0 0 20px rgba(34, 197, 94, 0.5); }\n",
    "\n",
    "#clear-btn {\n",
    "    background: #1e293b; color: #cbd5e1; border: 1px solid #334155;\n",
    "    height: 50px; border-radius: 10px; font-weight: 600;\n",
    "}\n",
    "#clear-btn:hover { color: white; border-color: white; }\n",
    "\"\"\"\n",
    "\n",
    "# ---------- 2. LOGIC ----------\n",
    "\n",
    "def _safe_filename(p):\n",
    "    try: return Path(p).name if p else \"unknown\"\n",
    "    except: return \"unknown\"\n",
    "\n",
    "def _format_evidence_html(chunks: List[Dict]) -> str:\n",
    "    if not chunks: return \"<div style='color:#cbd5e1; text-align:center;'>No evidence found.</div>\"\n",
    "    out = [\"<div style='max-height: 450px; overflow-y: auto; padding-right: 5px;'>\"]\n",
    "    for i, r in enumerate(chunks, 1):\n",
    "        sf = _safe_filename(r.get(\"source_file\"))\n",
    "        sc = r.get(\"rerank_score\", r.get(\"score\", 0.0))\n",
    "        txt = (r.get(\"text\") or \"\").strip().replace(\"\\n\", \" \")[:400] + \"...\"\n",
    "        out.append(f\"\"\"\n",
    "        <div class=\"evidence-card\">\n",
    "            <div style=\"display:flex; justify-content:space-between; margin-bottom:5px; color:#94a3b8; font-size:0.85em;\">\n",
    "                <span style=\"color:#ffffff; font-weight:bold;\">{i}. {sf}</span>\n",
    "                <span style=\"color:#4ade80;\">Score: {sc:.2f}</span>\n",
    "            </div>\n",
    "            <div style=\"color:#e2e8f0; font-size:0.9em; line-height:1.5;\">{txt}</div>\n",
    "        </div>\"\"\")\n",
    "    out.append(\"</div>\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "# üü¢ JAVASCRIPT TIMER SCRIPT\n",
    "# This tiny script finds the timer element and updates it every 100ms\n",
    "js_timer_script = \"\"\"\n",
    "<script>\n",
    "    function startLiveTimer() {\n",
    "        let startTime = Date.now();\n",
    "        let timerElement = document.getElementById('live-timer');\n",
    "        if(window.ragInterval) clearInterval(window.ragInterval);\n",
    "\n",
    "        window.ragInterval = setInterval(() => {\n",
    "            if(timerElement) {\n",
    "                let elapsed = ((Date.now() - startTime) / 1000).toFixed(1);\n",
    "                timerElement.innerText = elapsed + \"s\";\n",
    "            } else {\n",
    "                // Try to find element again if DOM updated\n",
    "                timerElement = document.getElementById('live-timer');\n",
    "            }\n",
    "        }, 100);\n",
    "    }\n",
    "    // Auto-start when this HTML is loaded\n",
    "    startLiveTimer();\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "def ui_answer(question: str, top_k: int, progress=gr.Progress()):\n",
    "    fused_topk = 60\n",
    "    per_query_k = 25\n",
    "    verify = True\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not question.strip():\n",
    "        yield gr.update(visible=False), gr.update(visible=False), \"\", \"\", \"\", \"\"\n",
    "        return\n",
    "\n",
    "    # PHASE 1\n",
    "    progress(0.1, desc=\"Scanning...\")\n",
    "    status_html = f\"\"\"\n",
    "    <div class=\"status-container phase-search\">\n",
    "        <div style=\"display:flex; align-items:center;\">\n",
    "            <div class=\"smart-spinner\" style=\"margin-right:15px;\"></div>\n",
    "            <span>SCANNING DATABASE...</span>\n",
    "        </div>\n",
    "        <span style=\"font-size:0.9em;\">PHASE 1/4 | <span id=\"live-timer\">0.0s</span></span>\n",
    "    </div>\n",
    "    {js_timer_script}\n",
    "    \"\"\"\n",
    "    yield (gr.update(visible=True), gr.update(value=\"...\"), \"\", \"\", \"\", status_html)\n",
    "\n",
    "    try:\n",
    "        retrieved, variants = fusion_retrieve(question, fused_topk=fused_topk, per_query_k=per_query_k)\n",
    "    except Exception as e:\n",
    "        print(f\"Search Error: {e}\")\n",
    "        retrieved = []\n",
    "        variants = [question]\n",
    "\n",
    "    if not retrieved:\n",
    "        yield (gr.update(visible=True), \"No docs found.\", \"\", \"\", \"\", \"<div class='status-container phase-done' style='border-color:red; color:red;'>‚ùå NO RESULTS</div>\")\n",
    "        return\n",
    "\n",
    "    # PHASE 2\n",
    "    progress(0.4, desc=\"Reranking...\")\n",
    "    # Note: We re-inject the timer span (id=\"live-timer\") but NOT the script again,\n",
    "    # because the script is already running and looking for that ID.\n",
    "    elapsed = time.time() - start_time\n",
    "    status_html = f\"\"\"\n",
    "    <div class=\"status-container phase-rerank\">\n",
    "        <div style=\"display:flex; align-items:center;\">\n",
    "            <div class=\"smart-spinner\" style=\"margin-right:15px;\"></div>\n",
    "            <span>FOUND {len(retrieved)} DOCS -> RERANKING...</span>\n",
    "        </div>\n",
    "        <span style=\"font-size:0.9em;\">PHASE 2/4 | <span id=\"live-timer\">{elapsed:.1f}s</span></span>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    yield (gr.update(visible=True), \"...\", \"\", \", \".join(variants), \"\", status_html)\n",
    "\n",
    "    try:\n",
    "        reranked = cross_encoder_rerank(question, retrieved, top_n=int(top_k))\n",
    "    except:\n",
    "        reranked = retrieved[:int(top_k)]\n",
    "\n",
    "    # PHASE 3\n",
    "    progress(0.7, desc=\"Thinking...\")\n",
    "    elapsed = time.time() - start_time\n",
    "    status_html = f\"\"\"\n",
    "    <div class=\"status-container phase-gen\">\n",
    "        <div style=\"display:flex; align-items:center;\">\n",
    "            <div class=\"smart-spinner\" style=\"margin-right:15px;\"></div>\n",
    "            <span>ANALYZING & WRITING...</span>\n",
    "        </div>\n",
    "        <span style=\"font-size:0.9em;\">PHASE 3/4 | <span id=\"live-timer\">{elapsed:.1f}s</span></span>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    yield (gr.update(visible=True), \"Generating...\", \"\", \", \".join(variants), \"\", status_html)\n",
    "\n",
    "    draft = generate_answer(question, reranked)\n",
    "    final = verifier_pass(question, draft, reranked) if verify else draft\n",
    "\n",
    "    # PHASE 4\n",
    "    progress(1.0, desc=\"Done!\")\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Stop timer script hack (clearing interval)\n",
    "    stop_script = \"<script>if(window.ragInterval) clearInterval(window.ragInterval);</script>\"\n",
    "\n",
    "    status_html = f\"\"\"\n",
    "    <div class=\"status-container phase-done\">\n",
    "        <div style=\"display:flex; align-items:center;\">\n",
    "            <span style=\"font-size:1.5em; margin-right:15px;\">‚úÖ</span>\n",
    "            <span>COMPLETE</span>\n",
    "        </div>\n",
    "        <span style=\"font-weight:bold;\">{total_time:.2f}s</span>\n",
    "    </div>\n",
    "    {stop_script}\n",
    "    \"\"\"\n",
    "\n",
    "    yield (\n",
    "        gr.update(visible=True),\n",
    "        final,\n",
    "        _format_evidence_html(reranked),\n",
    "        \", \".join(variants),\n",
    "        draft,\n",
    "        status_html\n",
    "    )\n",
    "\n",
    "# ---------- 3. LAYOUT ----------\n",
    "\n",
    "theme = gr.themes.Base(primary_hue=\"green\", neutral_hue=\"slate\").set(\n",
    "    body_background_fill=\"#020617\", block_background_fill=\"#0f172a\", block_border_width=\"0px\"\n",
    ")\n",
    "\n",
    "with gr.Blocks(theme=theme, css=green_mist_css, title=\"FoodRAG Pro\") as demo:\n",
    "\n",
    "    # TITLE\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.HTML(\"\"\"\n",
    "            <div style=\"padding: 30px 0;\">\n",
    "                <div class=\"header-title\">FoodRAG Pro</div>\n",
    "                <div class=\"header-subtitle\">Intelligent Nutritional Analysis</div>\n",
    "            </div>\n",
    "            \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # LEFT: INPUT\n",
    "        with gr.Column(scale=4):\n",
    "            gr.Markdown(\"### QUERY\")\n",
    "            q = gr.Textbox(placeholder=\"E.g., Can I take magnesium with antibiotics?\", lines=5, show_label=False)\n",
    "\n",
    "            gr.Markdown(\"### SETTINGS\")\n",
    "            top_k = gr.Slider(1, 10, value=5, step=1, label=\"Citations (K)\")\n",
    "\n",
    "            with gr.Row():\n",
    "                btn = gr.Button(\"Analyze\", variant=\"primary\", elem_id=\"search-btn\")\n",
    "                clear = gr.Button(\"Reset\", variant=\"secondary\", elem_id=\"clear-btn\")\n",
    "\n",
    "        # RIGHT: OUTPUT\n",
    "        with gr.Column(scale=6, visible=False) as output_col:\n",
    "            # Status Bar\n",
    "            status_display = gr.HTML()\n",
    "\n",
    "            gr.Markdown(\"### üß† ANSWER\")\n",
    "            answer = gr.Textbox(lines=8, show_label=False, interactive=False)\n",
    "\n",
    "            gr.Markdown(\"### üìÇ SOURCES\")\n",
    "            evidence = gr.HTML()\n",
    "\n",
    "            with gr.Accordion(\"üìù  Logs\", open=False):\n",
    "                variants_out = gr.Textbox(label=\"Expansion\", interactive=False)\n",
    "                draft_out = gr.Textbox(label=\"Draft\", lines=4, interactive=False)\n",
    "\n",
    "    # ACTIONS\n",
    "    btn.click(\n",
    "        ui_answer,\n",
    "        inputs=[q, top_k],\n",
    "        outputs=[output_col, answer, evidence, variants_out, draft_out, status_display],\n",
    "        show_progress=\"hidden\"\n",
    "    )\n",
    "    q.submit(\n",
    "        ui_answer,\n",
    "        inputs=[q, top_k],\n",
    "        outputs=[output_col, answer, evidence, variants_out, draft_out, status_display],\n",
    "        show_progress=\"hidden\"\n",
    "    )\n",
    "\n",
    "    def reset_ui():\n",
    "        return (\"\", 5, gr.update(visible=False), \"\", \"\", \"\", \"\", \"\")\n",
    "\n",
    "    clear.click(reset_ui, outputs=[q, top_k, output_col, answer, evidence, variants_out, draft_out, status_display])\n",
    "\n",
    "print(\" Launching Final UI (Live Timer + White Text)...\")\n",
    "demo.launch(share=False, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## STEP 11 ‚Äî Evaluation (retrieval)\n",
    "The goal of the evaluation is to measure the quality of the retrieval and reranking components of the RAG pipeline.\n",
    "\n",
    "Why MRR@10?\n",
    " - Works great when each query has 1+ relevant chunks\n",
    " - Rewards ranking the first relevant chunk as high as possible\n",
    " - Simple + very standard for retrieval/reranking evaluation\n",
    "\n",
    "Requirements:\n",
    " eval_questions.jsonl with:  {\"query\": \"...\", \"relevant_chunk_ids\": [\"chunk_id1\", \"chunk_id2\", ...]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 10 ‚Äî Retrieval Evaluation (MRR@10)\n",
    "# =========================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ---- config ----\n",
    "EVAL_PATH = Path(\"eval_questions.jsonl\")   # <-- make sure this file exists\n",
    "K = 10\n",
    "\n",
    "# ---- metric ----\n",
    "def mrr_at_k(ranked_ids, relevant_set, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank at K for a single query.\n",
    "    \"\"\"\n",
    "    for i, cid in enumerate(ranked_ids[:k], start=1):\n",
    "        if cid in relevant_set:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ---- sanity checks ----\n",
    "print(\"EVAL FILE PATH:\", EVAL_PATH.resolve())\n",
    "print(\"EVAL FILE EXISTS:\", EVAL_PATH.exists())\n",
    "print(\"EVAL FILE SIZE:\", EVAL_PATH.stat().st_size if EVAL_PATH.exists() else \"N/A\")\n",
    "\n",
    "\n",
    "def evaluate_mrr10(\n",
    "    eval_path: Path,\n",
    "    k: int = 10,\n",
    "    fused_topk: int = 60,\n",
    "    per_query_k: int = 25,\n",
    "    show_per_query: bool = True,\n",
    "):\n",
    "    print(\">>> START evaluate_mrr10\")\n",
    "\n",
    "    # load evaluation set\n",
    "    rows = [\n",
    "        json.loads(l)\n",
    "        for l in eval_path.read_text(\"utf-8\").splitlines()\n",
    "        if l.strip()\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    per_query_results = []\n",
    "\n",
    "    for r in rows:\n",
    "        q = r[\"query\"].strip()\n",
    "        relevant = set(r[\"relevant_chunk_ids\"])\n",
    "\n",
    "        # retrieval\n",
    "        retrieved, variants = fusion_retrieve(\n",
    "            q, fused_topk=fused_topk, per_query_k=per_query_k\n",
    "        )\n",
    "\n",
    "        # rerank (cross-encoder)\n",
    "        try:\n",
    "            reranked = cross_encoder_rerank(q, retrieved, top_n=k)\n",
    "        except Exception:\n",
    "            reranked = retrieved[:k]\n",
    "\n",
    "        ranked_ids = [x[\"chunk_id\"] for x in reranked]\n",
    "\n",
    "        score = mrr_at_k(ranked_ids, relevant, k)\n",
    "        scores.append(score)\n",
    "\n",
    "        first_rank = None\n",
    "        for i, cid in enumerate(ranked_ids[:k], start=1):\n",
    "            if cid in relevant:\n",
    "                first_rank = i\n",
    "                break\n",
    "\n",
    "        per_query_results.append({\n",
    "            \"query\": q,\n",
    "            \"variants\": variants,\n",
    "            \"mrr10\": score,\n",
    "            \"first_relevant_rank\": first_rank,\n",
    "            \"top10\": ranked_ids,\n",
    "        })\n",
    "\n",
    "    mean_mrr = float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "    print(\"\\n=== Retrieval Evaluation: MRR@10 ===\")\n",
    "    print(\"Queries:\", len(scores))\n",
    "    print(f\"MRR@10: {mean_mrr:.3f}\")\n",
    "\n",
    "    if show_per_query:\n",
    "        print(\"\\nPer-query preview (first 5):\")\n",
    "        for item in per_query_results[:5]:\n",
    "            rank_str = (\n",
    "                item[\"first_relevant_rank\"]\n",
    "                if item[\"first_relevant_rank\"] is not None\n",
    "                else \"NOT IN TOP10\"\n",
    "            )\n",
    "            print(\"-\" * 60)\n",
    "            print(\"Query:\", item[\"query\"])\n",
    "            print(\"Variants:\", item[\"variants\"])\n",
    "            print(f\"First relevant rank: {rank_str}\")\n",
    "            print(f\"MRR@10: {item['mrr10']:.3f}\")\n",
    "            print(\"Top 3:\", item[\"top10\"][:3])\n",
    "\n",
    "    return mean_mrr, per_query_results\n",
    "\n",
    "\n",
    "# ---- run evaluation ----\n",
    "print(\">>> CALLING evaluation now...\")\n",
    "mean_mrr10, details = evaluate_mrr10(EVAL_PATH, k=10)\n",
    "print(\">>> DONE. mean_mrr10 =\", mean_mrr10)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
