#%% md
# Food / Diet / Nutrition RAG (Multi‑Source)
### TXT + PDF) → Ingestion → Chunking → Index (Chroma) → Query Improve → Hybrid Retrieval → Cross‑Encoder Rerank → Self‑Check → Evaluation → UI
.

**Supported source types**
- **TXT** (`.txt`)
- **PDF** (`.pdf`) — extracted with PyMuPDF (fallback: pdfplumber)

**Folder convention** (recommended)
- `data/txt/` for TXT
- `data/pdf/` for PDFs

You can also put files directly under `data/` and the loader will still find them.

#%% md
## STEP 0 — Setup (PyCharm-friendly)
Ensures paths work even if the notebook is stored under `notebooks/`.
#%%

import os
from pathlib import Path
import importlib.util
import sys

cwd = Path.cwd()
if cwd.name.lower() == "notebooks":
    os.chdir(cwd.parent)

print("CWD:", Path.cwd())

#%% md
## STEP 1 — Install & Imports
Install dependencies (run once). The notebook has fallbacks when optional libs are missing.
#%%

# Uncomment if needed:
# !pip -q install -U sentence-transformers rank-bm25 chromadb pyspellchecker rapidfuzz ipywidgets nltk pymupdf pdfplumber odfpy

import re, json, math, time, hashlib
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import numpy as np

# Embeddings & reranker
try:
    from sentence_transformers import SentenceTransformer, CrossEncoder
except Exception:
    SentenceTransformer = None
    CrossEncoder = None

# Keyword retrieval
try:
    from rank_bm25 import BM25Okapi
except Exception:
    BM25Okapi = None

# Query improve utilities
try:
    from spellchecker import SpellChecker
except Exception:
    SpellChecker = None

try:
    from rapidfuzz import fuzz
except Exception:
    fuzz = None

# Vector DB persistence
try:
    import chromadb
    from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
except Exception:
    chromadb = None
    SentenceTransformerEmbeddingFunction = None

# UI
try:
    import ipywidgets as widgets
    from IPython.display import display, Markdown, clear_output
except Exception:
    widgets = None

# PDF extraction
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None

try:
    import pdfplumber
except Exception:
    pdfplumber = None

# ODF extraction
try:
    from odf.opendocument import load as odf_load
    from odf import text as odf_text, teletype as odf_teletype
except Exception:
    odf_load = None
    odf_text = None
    odf_teletype = None

print("Imports loaded")
print("SentenceTransformer:", bool(SentenceTransformer))
print("CrossEncoder:", bool(CrossEncoder))
print("BM25:", bool(BM25Okapi))
print("Chroma:", bool(chromadb))
print("ipywidgets:", bool(widgets))
print("PyMuPDF:", bool(fitz))
print("pdfplumber:", bool(pdfplumber))
print("odfpy:", bool(odf_load))

#%% md
## STEP 2 — Data Sources (separated by source type)
We keep files separate by source type (TXT/PDF/ODF) but also support searching under `data/`.
#%%

DATA_DIR = Path("data")
TXT_DIR = DATA_DIR / "txt"
PDF_DIR = DATA_DIR / "pdf"
ODF_DIR = DATA_DIR / "odf"

SUPPORTED = {
    "txt": [".txt"],
    "pdf": [".pdf"],
    "odf": [".odt", ".ods", ".odp"],
}

@dataclass
class DocFile:
    source_type: str
    path: str
    name: str

def discover_files() -> List[DocFile]:
    out: List[DocFile] = []
    roots = [
        ("txt", TXT_DIR),
        ("pdf", PDF_DIR),
        ("odf", ODF_DIR),
        # fallback: scan all under data
        ("_data", DATA_DIR),
    ]

    seen = set()
    for st, root in roots:
        if not root.exists():
            continue
        for fp in root.rglob("*"):
            if not fp.is_file():
                continue
            ext = fp.suffix.lower()

            # decide type
            if ext in SUPPORTED["txt"]:
                typ = "txt"
            elif ext in SUPPORTED["pdf"]:
                typ = "pdf"
            elif ext in SUPPORTED["odf"]:
                typ = "odf"
            else:
                continue

            key = str(fp.resolve())
            if key in seen:
                continue
            seen.add(key)
            out.append(DocFile(source_type=typ, path=str(fp), name=fp.name))
    return sorted(out, key=lambda x: (x.source_type, x.name))

doc_files = discover_files()
print("Total files:", len(doc_files))
for f in doc_files[:20]:
    print(f"- [{f.source_type}] {f.path}")

#%% md
## STEP 3 — Extraction (TXT / PDF / ODF)
Each source type is extracted into raw text.

- **TXT:** UTF‑8 decode
- **PDF:** PyMuPDF (best-effort), fallback to pdfplumber
#%%

def read_txt(fp: Path) -> str:
    return fp.read_text(encoding="utf-8", errors="replace")

def read_pdf_pymupdf(fp: Path) -> str:
    # Best-effort extraction (no OCR). If your PDFs are scanned images, you need OCR (not included here).
    if fitz is None:
        raise RuntimeError("PyMuPDF not available")
    doc = fitz.open(fp)
    pages = []
    for page in doc:
        pages.append(page.get_text("text"))
    doc.close()
    return "\n".join(pages)

def read_pdf_pdfplumber(fp: Path) -> str:
    if pdfplumber is None:
        raise RuntimeError("pdfplumber not available")
    pages = []
    with pdfplumber.open(fp) as pdf:
        for p in pdf.pages:
            pages.append(p.extract_text() or "")
    return "\n".join(pages)

def read_odf(fp: Path) -> str:
    if odf_load is None:
        raise RuntimeError("odfpy not available")
    doc = odf_load(str(fp))
    # Extract text from <text:p> elements (best-effort).
    ps = doc.getElementsByType(odf_text.P)
    out = []
    for p in ps:
        out.append(odf_teletype.extractText(p))
    return "\n".join(out)

def extract_text(doc: DocFile) -> str:
    fp = Path(doc.path)
    if doc.source_type == "txt":
        return read_txt(fp)
    if doc.source_type == "pdf":
        # prefer PyMuPDF, fallback to pdfplumber
        if fitz is not None:
            return read_pdf_pymupdf(fp)
        if pdfplumber is not None:
            return read_pdf_pdfplumber(fp)
        raise RuntimeError("No PDF extractor available (install pymupdf or pdfplumber).")
    if doc.source_type == "odf":
        return read_odf(fp)
    raise ValueError("Unknown source_type: " + doc.source_type)

# quick smoke test
if doc_files:
    t = extract_text(doc_files[0])
    print("First file:", doc_files[0].name, "| type:", doc_files[0].source_type)
    print("Chars:", len(t))
    print(t[:300], "...")
else:
    print("No files found under data/. Add files under data/txt, data/pdf, data/odf.")

#%% md
## STEP 4 — Cleaning + Metadata + Chunking
We normalize text, detect rough structure (paragraphs/sentences), then create overlapping chunks.

Metadata stored per chunk:
- `source_type`, `source_file`, `doc_id`
- `topic` (food/nutrition/unknown), `language`
- optional: `reliability`, `date`, `section`

#%%

def detect_language_light(text: str) -> str:
    return "he" if re.search(r"[\u0590-\u05FF]", text) else "en"

def clean_text(s: str) -> str:
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    s = re.sub(r"[“”]", '"', s)
    s = re.sub(r"[‘’]", "'", s)
    return s.strip()

def split_paragraphs(s: str) -> List[str]:
    return [p.strip() for p in s.split("\n\n") if p.strip()]

def split_sentences_best_effort(s: str) -> List[str]:
    parts = re.split(r"(?<=[.!?])\s+", s.strip())
    return [p.strip() for p in parts if p.strip()]

def guess_topic(text: str, filename: str) -> str:
    t = text.lower()
    fn = filename.lower()
    nutrition_terms = ["vitamin","calorie","protein","fiber","sodium","cholesterol","diet","nutrition","omega","macro","micronutrient"]
    food_terms = ["ingredients","recipe","bake","cook","boil","meal","serving","mix","stir"]
    if any(x in fn for x in ["nutrition","diet","health"]) or any(x in t for x in nutrition_terms):
        return "nutrition"
    if any(x in fn for x in ["recipe","cook","meal"]) or any(x in t for x in food_terms):
        return "food"
    return "unknown"

CHUNK_MAX_CHARS = 1200
CHUNK_OVERLAP_CHARS = 200

@dataclass
class Chunk:
    chunk_id: str
    doc_id: str
    source_type: str
    source_file: str
    text: str
    meta: Dict

def stable_id(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()[:10]

def recursive_chunk(paragraph: str, max_chars: int, overlap: int) -> List[str]:
    if len(paragraph) <= max_chars:
        return [paragraph]
    sents = split_sentences_best_effort(paragraph)
    chunks, cur = [], ""
    for sent in sents:
        if len(cur) + len(sent) + 1 <= max_chars:
            cur = (cur + " " + sent).strip()
        else:
            if cur:
                chunks.append(cur)
            cur = sent
    if cur:
        chunks.append(cur)
    if overlap > 0 and len(chunks) > 1:
        out, prev = [], ""
        for c in chunks:
            out.append((prev[-overlap:] + " " + c).strip() if prev else c)
            prev = c
        return out
    return chunks

def ingest_all(doc_files: List[DocFile]) -> List[Chunk]:
    out: List[Chunk] = []
    for doc in doc_files:
        raw = extract_text(doc)
        cleaned = clean_text(raw)
        doc_id = Path(doc.name).stem
        lang = detect_language_light(cleaned)
        topic = guess_topic(cleaned, doc.name)

        paras = split_paragraphs(cleaned)
        for pi, p in enumerate(paras):
            pieces = recursive_chunk(p, CHUNK_MAX_CHARS, CHUNK_OVERLAP_CHARS)
            for ci, piece in enumerate(pieces):
                cid = f"{doc_id}::{doc.source_type}::p{pi}::c{ci}::{stable_id(piece)}"
                out.append(Chunk(
                    chunk_id=cid,
                    doc_id=doc_id,
                    source_type=doc.source_type,
                    source_file=doc.path,
                    text=piece,
                    meta={
                        "source_type": doc.source_type,
                        "source_file": doc.path,
                        "topic": topic,
                        "language": lang,
                        "reliability": "unknown",
                        "date": None,
                        "section": None
                    }
                ))
    return out

chunks = ingest_all(doc_files)
print("Chunks:", len(chunks))
if chunks:
    print("Example:", chunks[0].chunk_id)
    print(chunks[0].text[:220], "...")

#%% md
## STEP 5 — Indexing (Chroma persistence + fallback)
We index chunks in a persistent vector DB (Chroma). If Chroma is unavailable, we compute an in-memory embedding matrix.
#%%
EMB_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
CHROMA_DIR = "chroma_food_rag"

if SentenceTransformer is None:
    raise RuntimeError("Install sentence-transformers to build embeddings.")

def chroma_safe_metadata(meta: dict) -> dict:
    """
    Chroma metadata values must be only: bool | int | float | str
    """
    safe = {}
    for k, v in meta.items():
        if v is None:
            continue  # drop Nones
        if isinstance(v, (bool, int, float, str)):
            safe[k] = v
        else:
            # convert anything else to string
            safe[k] = str(v)
    return safe
emb_model = SentenceTransformer(EMB_MODEL_NAME) if chunks else None

chroma_col = None
if chromadb is not None and SentenceTransformerEmbeddingFunction is not None and chunks:
    client = chromadb.PersistentClient(path=CHROMA_DIR)
    emb_fn = SentenceTransformerEmbeddingFunction(model_name=EMB_MODEL_NAME)
    chroma_col = client.get_or_create_collection(
        name="food_rag_chunks_multisource",
        embedding_function=emb_fn,
        metadata={"hnsw:space": "cosine"}
    )

    existing = set()
    try:
        existing = set(chroma_col.get(include=[]).get("ids", []))
    except Exception:
        existing = set()

    new_ids, new_docs, new_metas = [], [], []
for c in chunks:
    if c.chunk_id not in existing:
        new_ids.append(c.chunk_id)
        new_docs.append(c.text)
        new_metas.append(chroma_safe_metadata(c.meta))

if new_ids:
    chroma_col.add(ids=new_ids, documents=new_docs, metadatas=new_metas)
    print(f"Added {len(new_ids)} new chunks to Chroma")
else:
    print("Chroma already up to date")

vecs = None
if chroma_col is None and emb_model is not None and chunks:
    vecs = emb_model.encode([c.text for c in chunks], normalize_embeddings=True, show_progress_bar=True).astype(np.float32)
    print("vecs:", vecs.shape)

#%%
#Verify latest files: If Chunks in Python > Chunks in Chroma, then it’s not actually up to date (usually because chunk_ids changed).
print("Chunks in Python:", len(chunks))

if chroma_col is not None:
    count = len(chroma_col.get(include=[]).get("ids", []))
    print("Chunks in Chroma:", count)

#%% md
## STEP 6 — Hybrid Retrieval (BM25 + embeddings) + Query Improvement
We:
1) create query variants
2) retrieve using embeddings + BM25
3) fuse/deduplicate results

This step improves recall without exploding the query:
1) Normalize (lowercase, units)
2) Domain-safe spell correction (lightweight)
3) Synonym expansion using a real vocabulary (MeSH Entry Terms)
4) Produce 2–4 short query variants and fuse results
#%%
# Build a real synonym vocabulary from MeSH (one-time) + cache to JSON
# Uses lxml(recover=True) so big XML parsing won't crash.

from pathlib import Path
from collections import defaultdict
import json

# If this import fails, install into the SAME env as your Jupyter kernel:
# In terminal:  python -m pip install lxml
from lxml import etree

MESH_XML = Path("vocab/mesh_desc.xml")
CACHE = Path("vocab/mesh_synonyms_cache.json")
CACHE.parent.mkdir(parents=True, exist_ok=True)

def norm(t: str) -> str:
    return " ".join((t or "").lower().split())

def build_mesh_synonyms_cache(mesh_path: Path, cache_path: Path, max_terms_per_head: int = 25) -> dict:
    if not mesh_path.exists():
        raise FileNotFoundError(f"Missing MeSH XML at: {mesh_path.resolve()}")

    print("⏳ Parsing MeSH XML (this can take a few minutes once)...")
    parser = etree.XMLParser(recover=True, huge_tree=True)
    root = etree.parse(str(mesh_path), parser).getroot()

    syn_map = defaultdict(set)
    n_records = 0

    for rec in root.findall(".//DescriptorRecord"):
        n_records += 1
        head_el = rec.find("./DescriptorName/String")
        if head_el is None:
            continue
        head = norm(head_el.text)
        if not head:
            continue

        terms = []
        for term_el in rec.findall(".//ConceptList/Concept/TermList/Term/String"):
            s = norm(term_el.text)
            if s and s != head:
                terms.append(s)

        for s in terms[:max_terms_per_head]:
            syn_map[head].add(s)
            syn_map[s].add(head)

        if n_records % 5000 == 0:
            print(f"  processed {n_records} records...")

    syn_vocab = {k: sorted(v) for k, v in syn_map.items()}

    cache_path.write_text(json.dumps(syn_vocab, ensure_ascii=False), encoding="utf-8")
    print("Cached to:", cache_path.resolve())
    return syn_vocab

# Load cache if exists, otherwise build it
if CACHE.exists():
    syn_vocab = json.loads(CACHE.read_text("utf-8"))
    print("Loaded cached MeSH synonyms:", len(syn_vocab))
else:
    syn_vocab = build_mesh_synonyms_cache(MESH_XML, CACHE, max_terms_per_head=25)
    print("Built MeSH synonyms:", len(syn_vocab))

print("Example hypertension:", syn_vocab.get("hypertension", [])[:10])
print("Example vitamin c:", syn_vocab.get("vitamin c", [])[:10])

#%%

# BM25
def tokenize(text: str) -> List[str]:
    return re.findall(r"\w+", text.lower())

bm25 = None
if BM25Okapi is not None and chunks:
    bm25 = BM25Okapi([tokenize(c.text) for c in chunks])
    print("BM25 ready")
else:
    print("️BM25 not available")

SYNONYMS = {
    "hypertension": ["high blood pressure", "bp", "blood pressure"],
    "high blood pressure": ["hypertension", "bp"],
    "cholesterol": ["ldl", "hdl", "lipids"],
    "diabetes": ["dm", "blood sugar", "glucose"],
    "vitamin c": ["ascorbic acid", "vit c"],
    "vitamin d": ["d3", "cholecalciferol"],
    "omega 3": ["epa", "dha", "fish oil"],
    "fiber": ["dietary fiber", "roughage"],
    "salt": ["sodium", "na"],
    "weight loss": ["calorie deficit", "fat loss"],
    "low carb": ["keto", "ketogenic"],
}

def normalize_query(q: str) -> str:
    q = q.strip().lower()
    q = re.sub(r"\s+", " ", q)
    q = q.replace("kilogram", "kg").replace("kilograms", "kg")
    q = q.replace("milligram", "mg").replace("milligrams", "mg")
    q = q.replace("gram", "g").replace("grams", "g")
    return q

def light_spell_fix(q: str) -> str:
    if SpellChecker is None:
        return q
    sp = SpellChecker()
    toks = q.split()
    fixed = []
    for t in toks:
        fixed.append(t if len(t) <= 2 else (sp.correction(t) or t))
    return " ".join(fixed)

DOMAIN_EXPANSIONS = {
    # BP / hypertension nutrition intent
    "hypertension": ["dash diet", "low sodium diet", "dietary sodium reduction", "salt intake"],
    "high blood pressure": ["dash diet", "low sodium diet", "dietary sodium reduction", "salt intake"],
    "blood pressure": ["dash diet", "low sodium diet", "dietary sodium reduction", "salt intake"],

    # cholesterol nutrition intent
    "cholesterol": ["ldl cholesterol", "saturated fat", "dietary fiber", "plant sterols"],

    # diabetes nutrition intent
    "diabetes": ["glycemic index", "carbohydrate intake", "blood glucose", "insulin resistance"],
}
# ---------- Intent detection (lightweight, domain-focused) ----------
INTENT_RULES = [
    ("hypertension", ["hypertension", "blood pressure", "bp", "sodium", "salt"]),
    ("diabetes", ["diabetes", "glucose", "blood sugar", "insulin", "a1c", "t2d", "t1d"]),
    ("cholesterol", ["cholesterol", "ldl", "hdl", "triglycerides"]),
    ("weight_loss", ["weight loss", "fat loss", "calorie deficit", "bmi", "obesity", "overweight"]),
    ("vitamins", ["vitamin", "vit c", "vit d", "b12", "folate", "iron", "zinc", "magnesium", "omega"]),
    ("gut_health", ["gut", "microbiome", "probiotic", "prebiotic", "fiber", "constipation"]),
    ("sports", ["protein", "creatine", "muscle", "workout", "training", "recovery"]),
    ("general", []),
]

INTENT_HINTS = {
    "hypertension": ["low sodium", "dash diet", "dietary sodium"],
    "diabetes": ["low glycemic", "glycemic index", "carbohydrate intake"],
    "cholesterol": ["soluble fiber", "saturated fat", "plant sterols"],
    "weight_loss": ["high protein", "high fiber", "calorie density"],
    "vitamins": ["food sources", "recommended intake", "deficiency"],
    "gut_health": ["high fiber", "prebiotic foods", "fermented foods"],
    "sports": ["lean protein", "protein per day", "recovery nutrition"],
    "general": ["healthy diet", "balanced meals"],
}

def detect_intent(q: str) -> str:
    ql = q.lower()
    for intent, keys in INTENT_RULES:
        if any(k in ql for k in keys):
            return intent
    return "general"

# ---------- Variant quality filters ----------
BAD_PATTERNS = [
    r"\bhigh pressure,\s*blood\b",   # awkward inversion
    r"\bpressure,\s*blood\b",
]
BAD_WORDS = {"diastolic", "systolic"}  # too clinical, often hurts food retrieval

def is_good_variant(v: str) -> bool:
    vl = v.lower().strip()
    if any(re.search(p, vl) for p in BAD_PATTERNS):
        return False
    if any(w in vl for w in BAD_WORDS):
        return False
    # keep it reasonably short
    if len(vl.split()) > 14:
        return False
    return True

# ---------- Expansion: MeSH replacements + intent hint (1) ----------
def expand_query(q: str, max_alts: int = 4, syns_per_match: int = 2) -> list[str]:
    base = spell_fix_domain_safe(q)
    intent = detect_intent(base)

    alts = [base]

    # 1) MeSH synonym replacements (only for terms present in query grams)
    keys = sorted(candidate_keys_from_query(base), key=len, reverse=True)
    for key in keys:
        for s in syn_vocab.get(key, [])[:syns_per_match]:
            cand = base.replace(key, s)
            if is_good_variant(cand) and len(meaningful_tokens(cand)) >= 2:
                alts.append(cand)
        if len(alts) >= max_alts - 1:  # keep room for hint variant
            break

    # 2) Add ONE intent hint variant (general, not hypertension-only)
    hint_terms = INTENT_HINTS.get(intent, INTENT_HINTS["general"])
    hint = hint_terms[0] if hint_terms else None
    if hint:
        hint_variant = f"{base} {hint}"
        if is_good_variant(hint_variant):
            alts.append(hint_variant)

    # 3) Acronym expansion only if needed
    if "bp" in base and "blood pressure" not in base and len(alts) < max_alts:
        alts.append(base.replace("bp", "blood pressure"))

    # Dedupe + cap
    uniq, seen = [], set()
    for a in alts:
        a = normalize_query(a)
        if a and a not in seen and is_good_variant(a):
            uniq.append(a); seen.add(a)

    return uniq[:max_alts]

def expand_query(q: str, max_alts: int = 4, syns_per_match: int = 2) -> list[str]:
    base = spell_fix_domain_safe(q)
    alts = [base]

    # 1) MeSH synonym replacements (direct matches only)
    keys = sorted(candidate_keys_from_query(base), key=len, reverse=True)

    for key in keys:
        for s in syn_vocab.get(key, [])[:syns_per_match]:
            cand = base.replace(key, s)
            if len(meaningful_tokens(cand)) >= 2:
                alts.append(cand)
        if len(alts) >= max_alts:
            break

    # 2) Domain nutrition expansions (adds diet-related intent terms)
    # We append short expansions instead of rewriting the whole query.
    for trigger, extras in DOMAIN_EXPANSIONS.items():
        if trigger in base and len(alts) < max_alts:
            for e in extras:
                cand = f"{base} {e}"
                if len(meaningful_tokens(cand)) >= 2:
                    alts.append(cand)
                if len(alts) >= max_alts:
                    break

    # 3) Acronym expansion
    if "bp" in base and "blood pressure" not in base and len(alts) < max_alts:
        alts.append(base.replace("bp", "blood pressure"))

    # dedupe + cap
    uniq, seen = [], set()
    for a in alts:
        a = normalize_query(a)
        if a and a not in seen:
            uniq.append(a); seen.add(a)
    return uniq[:max_alts]


def minmax_norm(vals: List[float]) -> List[float]:
    if not vals:
        return []
    mn, mx = min(vals), max(vals)
    if mx - mn < 1e-9:
        return [1.0 for _ in vals]
    return [(v - mn) / (mx - mn) for v in vals]

def retrieve_chroma(q: str, top_k: int = 10, where: Optional[Dict]=None) -> List[Dict]:
    if chroma_col is None:
        return []
    res = chroma_col.query(
    query_texts=[q],
    n_results=top_k,
    where=where,
    include=["documents", "metadatas", "distances"]
)

    out = []
    for cid, doc, meta, dist in zip(res["ids"][0], res["documents"][0], res["metadatas"][0], res["distances"][0]):
        out.append({"chunk_id": cid, "text": doc, "meta": meta, "score": 1.0 - float(dist)})
    return out

def retrieve_embed_fallback(q: str, top_k: int = 10) -> List[Tuple[int, float]]:
    if vecs is None or emb_model is None:
        return []
    qv = emb_model.encode([q], normalize_embeddings=True, show_progress_bar=False).astype(np.float32)[0]
    sims = vecs @ qv
    idx = np.argsort(-sims)[:top_k]
    return [(int(i), float(sims[i])) for i in idx]

def retrieve_bm25(q: str, top_k: int = 10) -> List[Tuple[int, float]]:
    if bm25 is None:
        return []
    scores = bm25.get_scores(tokenize(q))
    idx = np.argsort(-scores)[:top_k]
    return [(int(i), float(scores[i])) for i in idx]

by_id = {c.chunk_id: c for c in chunks}

def fusion_retrieve(question: str, fused_topk: int = 25, per_query_k: int = 12, where: Optional[Dict]=None):
    variants = expand_query(question)
    fused: Dict[str, float] = {}

    for q in variants:
        # embeddings
        if chroma_col is not None:
            for r in retrieve_chroma(q, top_k=per_query_k, where=where):
                fused[r["chunk_id"]] = max(fused.get(r["chunk_id"], 0.0), 0.6 * float(r["score"]))
        else:
            emb = retrieve_embed_fallback(q, top_k=per_query_k)
            emb_scores = minmax_norm([s for _, s in emb])
            for (i, _), sc in zip(emb, emb_scores):
                fused[chunks[i].chunk_id] = max(fused.get(chunks[i].chunk_id, 0.0), 0.6 * sc)

        # bm25
        bm = retrieve_bm25(q, top_k=per_query_k)
        bm_scores = minmax_norm([s for _, s in bm])
        for (i, _), sc in zip(bm, bm_scores):
            fused[chunks[i].chunk_id] = max(fused.get(chunks[i].chunk_id, 0.0), 0.4 * sc)

    ranked = sorted(fused.items(), key=lambda x: x[1], reverse=True)[:fused_topk]
    results = []
    for cid, sc in ranked:
        c = by_id.get(cid)
        if not c:
            continue
        results.append({
            "chunk_id": c.chunk_id,
            "text": c.text,
            "meta": c.meta,
            "source_file": c.source_file,
            "source_type": c.source_type,
            "score": float(sc),
        })
    return results, variants

# Demo
q_demo = "foods for high blood pressure"
retrieved, variants = fusion_retrieve(q_demo, fused_topk=25)
print("Variants:", variants)
print("Top result:", retrieved[0]["chunk_id"] if retrieved else "—")

#%% md
## STEP 7 — Cross‑Encoder Reranking
Reranking usually gives the biggest precision boost by scoring (question, chunk_text) pairs directly.
#%%

RERANK_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"
reranker = CrossEncoder(RERANK_MODEL) if CrossEncoder is not None else None

def cross_encoder_rerank(question: str, retrieved: List[Dict], top_n: int = 10) -> List[Dict]:
    if reranker is None:
        return retrieved[:top_n]
    pairs = [(question, r["text"]) for r in retrieved]
    scores = reranker.predict(pairs)
    out = []
    for r, s in zip(retrieved, scores):
        r2 = dict(r)
        r2["rerank_score"] = float(s)
        out.append(r2)
    out.sort(key=lambda x: x.get("rerank_score", -1e9), reverse=True)
    return out[:top_n]

retrieved, variants = fusion_retrieve(q_demo, fused_topk=40)   # more candidates
reranked = cross_encoder_rerank(q_demo, retrieved, top_n=10)

for r in reranked[:3]:
    print(r["chunk_id"], "| type:", r["source_type"], "| rerank:", round(r.get("rerank_score", r["score"]), 3))

#%% md
## STEP 8 — Augmentation (prompt with citations)
We build a strict prompt:
- answer only from context
- if missing: "I don't know"
- end with citations (chunk IDs)
#%%

def build_context(chunks_list: List[Dict], max_chunks: int = 6, max_chars_each: int = 900) -> str:
    ctx = []
    for r in chunks_list[:max_chunks]:
        ctx.append(f"[{r['chunk_id']}] (type={r['source_type']}, source={Path(r['source_file']).name})\n{r['text'][:max_chars_each]}")
    return "\n\n".join(ctx)

def build_prompt(question: str, chunks_list: List[Dict]) -> str:
    context = build_context(chunks_list)
    return f"""You are a helpful assistant.
Answer the question ONLY using the provided context.
If the context does not contain the answer, say: "I don't know from the provided documents."

Question:
{question}

Context:
{context}

Rules:
- Use ONLY context facts.
- End with: Citations: [chunk_id1, chunk_id2, ...]
"""

prompt = build_prompt(q_demo, reranked)
print(prompt[:800], "...")

#%% md
## STEP 9 — Generation + Self‑Correction (verifier)
What this step does:
 1) Generates a DRAFT answer from retrieved context using Ollama
 2) Runs a VERIFIER pass that removes unsupported claims and enforces citations
 3) If Ollama is not running, it DOES NOT crash — it switches to retrieval-only mode

### Requirements: - Ollama installed and running : ollama pull llama3.2:3b
- A model pulled, e.g.:  ollama pull llama3.1:8b # or smaller if your laptop is weak: ollama pull llama3.2:3b
- Python package:pip install ollama

Notes:
- The verifier is essential for RAG: it reduces unsupported claims.
- If you have weak hardware, switch to a smaller model:    OLLAMA_MODEL = "llama3.2:3b"
#%%
import os
import urllib.request
from pathlib import Path
from typing import List, Dict, Tuple

OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://127.0.0.1:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "llama3.2:3b")  # smaller = faster

def ollama_available(host: str = OLLAMA_HOST) -> bool:
    try:
        urllib.request.urlopen(f"{host}/api/tags", timeout=1.5)
        return True
    except Exception:
        return False

USE_OLLAMA = ollama_available()

print("OLLAMA_HOST:", OLLAMA_HOST)
print("OLLAMA_MODEL:", OLLAMA_MODEL)
print("Ollama reachable" if USE_OLLAMA else "⚠️ Ollama NOT reachable -> Retrieval-only fallback (no crash)")

# Import ollama only if server is reachable (avoids confusion in some environments)
if USE_OLLAMA:
    import ollama
#%%
import re
from pathlib import Path
from typing import List, Dict

def build_context(chunks_list: List[Dict], max_chunks: int = 6, max_chars_each: int = 900) -> str:
    """
    Context block that the model must cite.
    We include the full chunk_id in brackets to make citations unambiguous.
    """
    ctx = []
    for r in chunks_list[:max_chunks]:
        ctx.append(
            f"[{r['chunk_id']}] (type={r.get('source_type','?')}, source={Path(r['source_file']).name})\n"
            f"{r['text'][:max_chars_each]}"
        )
    return "\n\n".join(ctx)

def allowed_chunk_ids(chunks_list: List[Dict], max_chunks: int = 6) -> List[str]:
    return [r["chunk_id"] for r in chunks_list[:max_chunks]]

def build_prompt(question: str, chunks_list: List[Dict]) -> str:
    ctx = build_context(chunks_list, max_chunks=6, max_chars_each=1100)
    valid = allowed_chunk_ids(chunks_list, max_chunks=6)

    return f"""You are a helpful assistant in a RAG system.

Question:
{question}

Context (ONLY source of truth):
{ctx}

STRICT RULES:
- Use ONLY facts explicitly stated in the context.
- If the context does not contain the answer, say:
  I don't know from the provided documents.
- When you write bullets, EACH bullet MUST end with exactly one citation in this format:
  - ... [FULL_CHUNK_ID]
- You may cite only from this allowed list:
{valid}
- Do NOT add any uncited sentence.

Output format:
- 1 short answer sentence
- 1–5 bullets (only if supported)
- Final line: Citations: [comma-separated FULL_CHUNK_IDs you used]
"""


def extract_citations(text: str) -> List[str]:
    m = re.search(r"Citations:\s*\[(.*?)\]\s*$", text.strip(), flags=re.IGNORECASE | re.DOTALL)
    if not m:
        return []
    inside = m.group(1).strip()
    if not inside:
        return []
    # split by comma
    return [c.strip().strip("'\"") for c in inside.split(",") if c.strip()]

def enforce_valid_citations(answer: str, valid_ids: List[str]) -> str:
    """
    If model outputs partial or invalid citations, replace them with [] to avoid fake grounding.
    """
    valid_set = set(valid_ids)
    cited = extract_citations(answer)
    if not cited:
        # if no citations, force empty citations (safe)
        if "Citations:" not in answer:
            return answer.rstrip() + "\nCitations: []"
        return answer

    # keep only valid full ids
    kept = [c for c in cited if c in valid_set]
    # remove invalid citations
    answer_wo = re.sub(r"\n?Citations:\s*\[.*?\]\s*$", "", answer.strip(), flags=re.IGNORECASE | re.DOTALL).strip()

    return answer_wo + "\nCitations: [" + ", ".join(kept) + "]"

def verifier_pass_strict(question: str, draft: str, chunks_list: List[Dict]) -> str:
    valid_ids = allowed_chunk_ids(chunks_list, max_chunks=6)
    ctx = build_context(chunks_list, max_chunks=6, max_chars_each=1100)

    verify_prompt = f"""You are a strict verifier for a RAG system.

Question:
{question}

Context:
{ctx}

Draft answer:
{draft}

STRICT RULES:
1) Remove ANY statement not explicitly supported by the context.
2) Remove ANY sentence or bullet that does not have a valid citation.
3) EACH bullet must end with: [FULL_CHUNK_ID]
4) Allowed citations (ONLY these):
{valid_ids}
5) Final line must be: Citations: [id1, id2, ...]

If not enough info, output exactly:
I don't know from the provided documents.
Citations: []
"""
    final = ollama_chat(verify_prompt, temperature=0.0)

    # Hard enforcement of valid citation IDs in final Citations list
    final = enforce_valid_citations(final, valid_ids)

    # Extra hard check: remove bullets missing [FULL_CHUNK_ID]
    lines = final.splitlines()
    cleaned = []
    bullet_re = re.compile(r"^\s*[\-\*]\s+.*\[[^\]]+\]\s*$")
    for line in lines:
        if line.strip().startswith(("-", "*")):
            if bullet_re.match(line.strip()):
                cleaned.append(line)
            else:
                # drop uncited bullet
                continue
        else:
            cleaned.append(line)

    return "\n".join(cleaned).strip()

#%%
q_demo = "foods for high blood pressure"

retrieved, variants = fusion_retrieve(q_demo, fused_topk=60, per_query_k=25)
reranked = cross_encoder_rerank(q_demo, retrieved, top_n=10)

draft = ollama_chat(build_prompt(q_demo, reranked), temperature=0.2)
final = verifier_pass(q_demo, draft, reranked)

print("Variants:", variants)
print("\n--- FINAL ---\n", final)

#%% md
## STEP 10 — Evaluation (retrieval)
The goal of the evaluation is to measure the quality of the retrieval and reranking components of the RAG pipeline.
  METRIC EVALUATION: MRR@10
 Why MRR@10?
 - Works great when each query has 1+ relevant chunks
 - Rewards ranking the first relevant chunk as high as possible
 - Simple + very standard for retrieval/reranking evaluation

Requirements:
 eval_questions.jsonl with:  {"query": "...", "relevant_chunk_ids": ["chunk_id1", "chunk_id2", ...]}

#%%
import json
from pathlib import Path
import numpy as np

EVAL_PATH = Path("eval_questions.jsonl")
K = 10

def mrr_at_k(ranked_ids, relevant_set, k: int) -> float:
    for i, cid in enumerate(ranked_ids[:k], start=1):
        if cid in relevant_set:
            return 1.0 / i
    return 0.0

print("EVAL FILE PATH:", eval_path.resolve())
print("EVAL FILE EXISTS:", eval_path.exists())
print("EVAL FILE SIZE:", eval_path.stat().st_size if eval_path.exists() else "N/A")

def evaluate_mrr10(eval_path: Path, k=10, fused_topk=60, per_query_k=25, show_per_query=True):
    print(">>> START evaluate_mrr10")  # debug line (should always appear)

    rows = [json.loads(l) for l in eval_path.read_text("utf-8").splitlines() if l.strip()]
    indexed_ids = set(c.chunk_id for c in chunks)

    scores = []
    per_query_results = []

    for r in rows:
        q = r["query"].strip()
        relevant = set(r["relevant_chunk_ids"])

        retrieved, variants = fusion_retrieve(q, fused_topk=fused_topk, per_query_k=per_query_k)
        reranked = cross_encoder_rerank(q, retrieved, top_n=k)
        ranked_ids = [x["chunk_id"] for x in reranked]

        score = mrr_at_k(ranked_ids, relevant, k)
        scores.append(score)

        first_rank = None
        for i, cid in enumerate(ranked_ids[:k], start=1):
            if cid in relevant:
                first_rank = i
                break

        per_query_results.append({
            "query": q,
            "variants": variants,
            "mrr10": score,
            "first_relevant_rank": first_rank,
            "top10": ranked_ids
        })

    print("Loaded evaluation queries:", len(rows))

    mean_mrr = float(np.mean(scores)) if scores else 0.0

    print(f"\n=== Retrieval Evaluation: MRR@{k} ===")
    print(f"Queries: {len(scores)}")
    print(f"MRR@{k}: {mean_mrr:.3f}\n")

    if show_per_query:
        print("Per-query preview (first 5):")
        for item in per_query_results[:5]:
            rank_str = item["first_relevant_rank"] if item["first_relevant_rank"] is not None else "NOT IN TOP10"
            print("-" * 60)
            print("Query:", item["query"])
            print("Variants:", item["variants"])
            print(f"First relevant rank: {rank_str} | MRR@{k}: {item['mrr10']:.3f}")
            print("Top 3:", item["top10"][:3])

    return mean_mrr, per_query_results

# --- run ---
print(">>> CALLING evaluation now...")  # debug line
mean_mrr10, details = evaluate_mrr10(EVAL_PATH, k=10, fused_topk=60, per_query_k=25, show_per_query=True)
print(">>> DONE. mean_mrr10 =", mean_mrr10)  # debug line


#%% md
## STEP 11 — UI (ipywidgets)
A demo UI: query box + filters by source type/topic/language + TopK + expandable chunks + prompt preview + answer box.
#%%

if widgets is None:
    print("ipywidgets not installed. Install it in STEP 1.")
else:
    q_box = widgets.Text(value="foods for high blood pressure", description="Query:", layout=widgets.Layout(width="48%"))
    topk = widgets.IntSlider(value=10, min=5, max=20, step=1, description="TopK:", continuous_update=False)

    source_dd = widgets.Dropdown(options=["any", "txt", "pdf", "odf"], value="any", description="Source:")
    topic_dd  = widgets.Dropdown(options=["any", "nutrition", "food", "unknown"], value="any", description="Topic:")
    lang_dd   = widgets.Dropdown(options=["any", "en", "he"], value="any", description="Lang:")

    btn = widgets.Button(description="Search", button_style="primary")
    out = widgets.Output()

    def build_where():
        # Works only with Chroma metadata filtering
        if chroma_col is None:
            return None
        where = {}
        if source_dd.value != "any":
            where["source_type"] = source_dd.value
        if topic_dd.value != "any":
            where["topic"] = topic_dd.value
        if lang_dd.value != "any":
            where["language"] = lang_dd.value
        return where if where else None

    def on_click(_):
        with out:
            clear_output()
            q = q_box.value.strip()
            if not q:
                display(Markdown("**Please enter a query.**"))
                return

            where = build_where()
            retrieved, variants = fusion_retrieve(q, fused_topk=max(25, topk.value), where=where)
            reranked = cross_encoder_rerank(q, retrieved, top_n=topk.value)

            display(Markdown(f"### Query variants\n`{variants}`"))
            display(Markdown("### Top chunks (expand)"))

            items, titles = [], []
            for i, r in enumerate(reranked, start=1):
                titles.append(
                    f"#{i} score={r.get('rerank_score', r['score']):.3f} | {r['source_type']} | {Path(r['source_file']).name} | {r['chunk_id']}"
                )
                items.append(widgets.Textarea(value=r["text"], layout=widgets.Layout(width="100%", height="130px")))

            if items:
                acc = widgets.Accordion(children=items)
                for i, t in enumerate(titles):
                    acc.set_title(i, t)
                display(acc)
            else:
                display(Markdown("No results found."))

            prompt = build_prompt(q, reranked)
            display(Markdown("### Prompt (preview)"))
            display(widgets.Textarea(value=prompt[:2500], layout=widgets.Layout(width="100%", height="220px")))

            draft, _ = generate_answer(q, reranked)
            final = verifier_stub(q, draft, reranked)

            display(Markdown("### Answer (stub + verifier)"))
            display(widgets.Textarea(value=final, layout=widgets.Layout(width="100%", height="120px")))

    btn.on_click(on_click)
    display(widgets.VBox([widgets.HBox([q_box, topk, source_dd, topic_dd, lang_dd, btn]), out]))
