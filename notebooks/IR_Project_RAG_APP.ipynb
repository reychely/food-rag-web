{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/reychely/food-rag-web/blob/main/IR_Project_RAG_APP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "b4fa51ee90eb46cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T11:12:39.279753Z",
     "start_time": "2026-01-17T11:12:39.201763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "nb = {\"cells\": [], \"metadata\": {\"kernelspec\": {\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\n",
    "                               \"language_info\": {\"name\":\"python\",\"version\":\"3.x\"}},\n",
    "      \"nbformat\": 4, \"nbformat_minor\": 5}\n",
    "\n",
    "def md(s): nb[\"cells\"].append({\"cell_type\":\"markdown\",\"metadata\":{},\"source\":s})\n",
    "def code(s): nb[\"cells\"].append({\"cell_type\":\"code\",\"metadata\":{},\"execution_count\":None,\"outputs\":[], \"source\":s})\n",
    "\n",
    "md(\"# Food / Diet / Nutrition RAG\\n\"\n",
    "\"### TXT Ingestion → Hybrid Retrieval → Cross‑Encoder Reranking → Self‑Correction → Evaluation → Notebook UI\\n\\n\"\n",
    "\"This notebook is written for **readers** (course staff / reviewers). Each step explains **why** it exists and **what** it does.\\n\\n\"\n",
    "\"**What’s improved vs. a basic RAG notebook**\\n\"\n",
    "\"- Hybrid retrieval (BM25 + embeddings)\\n\"\n",
    "\"- Multi-query expansion + fusion\\n\"\n",
    "\"- Cross‑encoder reranking (precision boost)\\n\"\n",
    "\"- Self‑correction verifier pass (reduces hallucinations)\\n\"\n",
    "\"- Persistence with a DB API (Chroma) so you don’t re-index every run\\n\"\n",
    "\"- Better notebook UI (filters + citations + expandable chunks)\\n\"\n",
    "\"- Retrieval evaluation (Hit@K, MRR) + optional generation faithfulness checks\\n\")\n",
    "\n",
    "md(\"## STEP 0 — Install & Imports\\n\"\n",
    "\"Install only what you need. The notebook runs even if some optional packages are missing, but best results require them.\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "# If you are on Google Colab, uncomment:\n",
    "# !pip -q install -U sentence-transformers rank-bm25 chromadb pyspellchecker rapidfuzz ipywidgets nltk\n",
    "\n",
    "import os, re, json, math, time, hashlib\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Retrieval / NLP\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "except Exception:\n",
    "    SentenceTransformer = None\n",
    "    CrossEncoder = None\n",
    "\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "except Exception:\n",
    "    BM25Okapi = None\n",
    "\n",
    "try:\n",
    "    from spellchecker import SpellChecker\n",
    "except Exception:\n",
    "    SpellChecker = None\n",
    "\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "except Exception:\n",
    "    fuzz = None\n",
    "\n",
    "# Optional synonyms\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "except Exception:\n",
    "    nltk = None\n",
    "    wn = None\n",
    "\n",
    "# DB API (persistence)\n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "except Exception:\n",
    "    chromadb = None\n",
    "    SentenceTransformerEmbeddingFunction = None\n",
    "\n",
    "# Notebook UI\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, Markdown, clear_output\n",
    "except Exception:\n",
    "    widgets = None\n",
    "\n",
    "print(\"✅ Imports loaded\")\n",
    "print(\"sentence-transformers:\", bool(SentenceTransformer))\n",
    "print(\"CrossEncoder:\", bool(CrossEncoder))\n",
    "print(\"BM25:\", bool(BM25Okapi))\n",
    "print(\"Chroma:\", bool(chromadb))\n",
    "print(\"SpellChecker:\", bool(SpellChecker))\n",
    "print(\"ipywidgets:\", bool(widgets))\n",
    "print(\"WordNet:\", bool(wn))\n",
    "\"\"\"))\n",
    "\n",
    "md(\"## STEP 1 — Data Sources (API → Document DB)\\n\"\n",
    "\"**Goal:** make your RAG pipeline work like a real application:\\n\"\n",
    "\"- Documents live in a \\\"DB\\\" (folder, Drive, S3, etc.)\\n\"\n",
    "\"- Ingestion code talks to an **API interface**\\n\\n\"\n",
    "\"In this project we implement a simple **Local Folder API** and keep a placeholder for Google Drive.\\n\"\n",
    "\"If you already use Google Drive in your existing notebook, you can swap the `LocalFolderSource` with your Drive implementation.\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "DATA_DIR = Path(\"data\")\n",
    "TXT_DIR = DATA_DIR / \"txt\"      # recommended\n",
    "FALLBACK_TXT_DIR = DATA_DIR     # fallback if you keep txt directly under ./data\n",
    "\n",
    "@dataclass\n",
    "class DocFile:\n",
    "    source_type: str      # \"txt\" now, later \"pdf\"/\"web\"\n",
    "    path: str             # string path or remote id\n",
    "    name: str\n",
    "    bytes_data: bytes     # raw bytes (API style)\n",
    "\n",
    "class DocumentSourceAPI:\n",
    "    \\\"\\\"\\\"Minimal interface for any future source: local folder, Google Drive, S3, etc.\\\"\\\"\\\"\n",
    "    def list_files(self) -> List[DocFile]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LocalFolderSource(DocumentSourceAPI):\n",
    "    def __init__(self, folder: Path):\n",
    "        self.folder = folder\n",
    "\n",
    "    def list_files(self) -> List[DocFile]:\n",
    "        files = []\n",
    "        if not self.folder.exists():\n",
    "            return files\n",
    "        for fp in sorted(self.folder.rglob(\"*.txt\")):\n",
    "            files.append(DocFile(\n",
    "                source_type=\"txt\",\n",
    "                path=str(fp),\n",
    "                name=fp.name,\n",
    "                bytes_data=fp.read_bytes()\n",
    "            ))\n",
    "        return files\n",
    "\n",
    "# Choose which folder you use\n",
    "source = LocalFolderSource(TXT_DIR if TXT_DIR.exists() else FALLBACK_TXT_DIR)\n",
    "doc_files = source.list_files()\n",
    "\n",
    "print(\"Found files:\", len(doc_files))\n",
    "for f in doc_files[:10]:\n",
    "    print(\"-\", f.name, \"|\", f.path)\n",
    "\"\"\"))\n",
    "\n",
    "md(\"## STEP 2 — Ingestion (TXT)\\n\"\n",
    "\"**Why:** raw text is messy. A consistent ingestion pipeline improves retrieval quality.\\n\\n\"\n",
    "\"We do:\\n\"\n",
    "\"1) UTF‑8 decoding (safe for mixed languages)\\n\"\n",
    "\"2) Cleaning (whitespace, punctuation normalization)\\n\"\n",
    "\"3) Structure detection: paragraphs by double newline; long paragraphs → sentence packing\\n\"\n",
    "\"4) Metadata per chunk: `source_file, topic, language, reliability, ...`\\n\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "def detect_language_light(text: str) -> str:\n",
    "    # very light heuristic: Hebrew => he, else en\n",
    "    if re.search(r\"[\\\\u0590-\\\\u05FF]\", text):\n",
    "        return \"he\"\n",
    "    return \"en\"\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.replace(\"\\\\r\\\\n\", \"\\\\n\").replace(\"\\\\r\", \"\\\\n\")\n",
    "    s = re.sub(r\"[ \\\\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\\\n{3,}\", \"\\\\n\\\\n\", s)\n",
    "    s = re.sub(r\"[“”]\", '\"', s)\n",
    "    s = re.sub(r\"[‘’]\", \"'\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def split_paragraphs(s: str) -> List[str]:\n",
    "    return [p.strip() for p in s.split(\"\\\\n\\\\n\") if p.strip()]\n",
    "\n",
    "def split_sentences_best_effort(s: str) -> List[str]:\n",
    "    # practical sentence split: .!? + whitespace\n",
    "    parts = re.split(r\"(?<=[.!?])\\\\s+\", s.strip())\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def guess_topic(text: str, filename: str) -> str:\n",
    "    # simple rules (extend if you want)\n",
    "    name = filename.lower()\n",
    "    t = text.lower()\n",
    "    if any(k in name for k in [\"nutrition\", \"diet\", \"health\"]) or any(k in t for k in [\"vitamin\", \"calorie\", \"protein\", \"fiber\"]):\n",
    "        return \"nutrition\"\n",
    "    if any(k in name for k in [\"recipe\", \"cook\", \"meal\"]) or any(k in t for k in [\"ingredients\", \"bake\", \"boil\"]):\n",
    "        return \"food\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def estimate_reliability(source_type: str, filename: str) -> str:\n",
    "    # For curated course projects, you can set this manually.\n",
    "    # Example heuristic:\n",
    "    if \"wiki\" in filename.lower():\n",
    "        return \"medium\"\n",
    "    return \"unknown\"\n",
    "\"\"\"))\n",
    "\n",
    "md(\"## STEP 3 — Chunking (recursive + overlap)\\n\"\n",
    "\"**Why overlap matters:** it prevents important details from being split across chunk boundaries.\\n\\n\"\n",
    "\"Design choices:\\n\"\n",
    "\"- Not too small (context loss)\\n\"\n",
    "\"- Not too big (dilution)\\n\"\n",
    "\"- Chunk IDs are stable and used as citations\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "CHUNK_MAX_CHARS = 1200\n",
    "CHUNK_OVERLAP_CHARS = 200\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    source_type: str\n",
    "    source_file: str\n",
    "    text: str\n",
    "    meta: Dict\n",
    "\n",
    "def recursive_chunk(paragraph: str, max_chars: int, overlap: int) -> List[str]:\n",
    "    if len(paragraph) <= max_chars:\n",
    "        return [paragraph]\n",
    "\n",
    "    sents = split_sentences_best_effort(paragraph)\n",
    "    chunks, cur = [], \"\"\n",
    "    for sent in sents:\n",
    "        if len(cur) + len(sent) + 1 <= max_chars:\n",
    "            cur = (cur + \" \" + sent).strip()\n",
    "        else:\n",
    "            if cur:\n",
    "                chunks.append(cur)\n",
    "            cur = sent\n",
    "    if cur:\n",
    "        chunks.append(cur)\n",
    "\n",
    "    if overlap > 0 and len(chunks) > 1:\n",
    "        out, prev = [], \"\"\n",
    "        for c in chunks:\n",
    "            out.append((prev[-overlap:] + \" \" + c).strip() if prev else c)\n",
    "            prev = c\n",
    "        return out\n",
    "    return chunks\n",
    "\n",
    "def stable_id(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:10]\n",
    "\n",
    "def ingest_to_chunks(files: List[DocFile]) -> List[Chunk]:\n",
    "    out: List[Chunk] = []\n",
    "    for f in files:\n",
    "        # UTF-8 decode\n",
    "        raw = f.bytes_data.decode(\"utf-8\", errors=\"replace\")\n",
    "        cleaned = clean_text(raw)\n",
    "\n",
    "        doc_id = Path(f.name).stem\n",
    "        lang = detect_language_light(cleaned)\n",
    "        topic = guess_topic(cleaned, f.name)\n",
    "        reliability = estimate_reliability(f.source_type, f.name)\n",
    "\n",
    "        paras = split_paragraphs(cleaned)\n",
    "        for pi, p in enumerate(paras):\n",
    "            pieces = recursive_chunk(p, CHUNK_MAX_CHARS, CHUNK_OVERLAP_CHARS)\n",
    "            for ci, piece in enumerate(pieces):\n",
    "                cid = f\"{doc_id}::p{pi}::c{ci}::{stable_id(piece)}\"\n",
    "                out.append(Chunk(\n",
    "                    chunk_id=cid,\n",
    "                    doc_id=doc_id,\n",
    "                    source_type=f.source_type,\n",
    "                    source_file=f.path,\n",
    "                    text=piece,\n",
    "                    meta={\n",
    "                        \"source_file\": f.path,\n",
    "                        \"source_type\": f.source_type,\n",
    "                        \"topic\": topic,\n",
    "                        \"language\": lang,\n",
    "                        \"reliability\": reliability,\n",
    "                        \"section\": None,\n",
    "                        \"date\": None\n",
    "                    }\n",
    "                ))\n",
    "    return out\n",
    "\n",
    "chunks = ingest_to_chunks(doc_files)\n",
    "print(\"Chunks:\", len(chunks))\n",
    "print(\"Sample:\", chunks[0].chunk_id if chunks else \"—\")\n",
    "\"\"\"))\n",
    "\n",
    "md(\"## STEP 4 — Indexing with a DB API (Chroma persistence)\\n\"\n",
    "\"**Why:** If your corpus grows, rebuilding indexes every run becomes slow.\\n\\n\"\n",
    "\"We use **Chroma** as a local persistent vector DB:\\n\"\n",
    "\"- stores embeddings + documents + metadata\\n\"\n",
    "\"- supports metadata filters (topic/language)\\n\\n\"\n",
    "\"If Chroma isn’t installed, the notebook falls back to in‑memory embeddings.\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHROMA_DIR = \"chroma_food_rag\"\n",
    "\n",
    "def get_embedder():\n",
    "    if SentenceTransformer is None:\n",
    "        raise RuntimeError(\"Install sentence-transformers in STEP 0.\")\n",
    "    return SentenceTransformer(EMB_MODEL_NAME)\n",
    "\n",
    "emb_model = get_embedder() if chunks else None\n",
    "\n",
    "def build_chroma_collection(chunks: List[Chunk]):\n",
    "    if chromadb is None or SentenceTransformerEmbeddingFunction is None:\n",
    "        return None\n",
    "\n",
    "    client = chromadb.PersistentClient(path=CHROMA_DIR)\n",
    "    emb_fn = SentenceTransformerEmbeddingFunction(model_name=EMB_MODEL_NAME)\n",
    "\n",
    "    col = client.get_or_create_collection(\n",
    "        name=\"food_rag_chunks\",\n",
    "        embedding_function=emb_fn,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "    )\n",
    "\n",
    "    # Upsert only missing ids (fast incremental)\n",
    "    existing = set()\n",
    "    try:\n",
    "        existing = set(col.get(include=[]).get(\"ids\", []))\n",
    "    except Exception:\n",
    "        existing = set()\n",
    "\n",
    "    new_ids, new_docs, new_metas = [], [], []\n",
    "    for c in chunks:\n",
    "        if c.chunk_id not in existing:\n",
    "            new_ids.append(c.chunk_id)\n",
    "            new_docs.append(c.text)\n",
    "            new_metas.append(c.meta)\n",
    "\n",
    "    if new_ids:\n",
    "        col.add(ids=new_ids, documents=new_docs, metadatas=new_metas)\n",
    "        print(f\"✅ Added {len(new_ids)} new chunks to Chroma\")\n",
    "    else:\n",
    "        print(\"✅ Chroma already up to date (no new chunks)\")\n",
    "\n",
    "    return col\n",
    "\n",
    "chroma_col = build_chroma_collection(chunks)\n",
    "print(\"Chroma ready:\", bool(chroma_col))\n",
    "\"\"\"))\n",
    "\n",
    "md(\"## STEP 5 — Hybrid Retrieval (BM25 + embeddings)\\n\"\n",
    "\"**Why hybrid:**\\n\"\n",
    "\"- BM25 helps exact terms (nutrient names, measurements)\\n\"\n",
    "\"- embeddings help semantic matches (\\\"foods for BP\\\" → \\\"hypertension\\\")\\n\\n\"\n",
    "\"We then combine them with **fusion** across multiple query variants.\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "# --- BM25 setup (in-memory) ---\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"\\\\w+\", text.lower())\n",
    "\n",
    "bm25 = None\n",
    "if BM25Okapi is not None and chunks:\n",
    "    bm25 = BM25Okapi([tokenize(c.text) for c in chunks])\n",
    "\n",
    "# --- Embedding matrix (fallback when Chroma not available) ---\n",
    "vecs = None\n",
    "if chromadb is None and emb_model is not None and chunks:\n",
    "    vecs = emb_model.encode([c.text for c in chunks], normalize_embeddings=True, show_progress_bar=True).astype(np.float32)\n",
    "\n",
    "def minmax_norm(vals: List[float]) -> List[float]:\n",
    "    if not vals:\n",
    "        return []\n",
    "    mn, mx = min(vals), max(vals)\n",
    "    if mx - mn < 1e-9:\n",
    "        return [1.0 for _ in vals]\n",
    "    return [(v - mn) / (mx - mn) for v in vals]\n",
    "\n",
    "def retrieve_bm25(q: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "    if bm25 is None:\n",
    "        return []\n",
    "    scores = bm25.get_scores(tokenize(q))\n",
    "    idx = np.argsort(-scores)[:top_k]\n",
    "    return [(int(i), float(scores[i])) for i in idx]\n",
    "\n",
    "def retrieve_embed_fallback(q: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "    if vecs is None or emb_model is None:\n",
    "        return []\n",
    "    qv = emb_model.encode([q], normalize_embeddings=True, show_progress_bar=False).astype(np.float32)[0]\n",
    "    sims = vecs @ qv\n",
    "    idx = np.argsort(-sims)[:top_k]\n",
    "    return [(int(i), float(sims[i])) for i in idx]\n",
    "\n",
    "def retrieve_chroma(q: str, top_k: int = 10, where: Optional[Dict]=None):\n",
    "    if chroma_col is None:\n",
    "        return []\n",
    "    res = chroma_col.query(query_texts=[q], n_results=top_k, where=where, include=[\"documents\",\"metadatas\",\"distances\",\"ids\"])\n",
    "    out = []\n",
    "    for cid, doc, meta, dist in zip(res[\"ids\"][0], res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
    "        # Chroma returns distance; for cosine, lower is better. Convert to similarity-ish score:\n",
    "        score = 1.0 - float(dist)\n",
    "        out.append({\"chunk_id\": cid, \"text\": doc, \"meta\": meta, \"score\": score})\n",
    "    return out\n",
    "\"\"\"))\n",
    "\n",
    "md(\"## STEP 6 — Query Improving (nutrition-focused)\\n\"\n",
    "\"To improve recall, we generate **2–4 alternative queries**:\\n\"\n",
    "\"- normalization (units)\\n\"\n",
    "\"- lightweight spell correction\\n\"\n",
    "\"- nutrition synonym expansion (domain dictionary)\\n\"\n",
    "\"- optional WordNet + fuzzy matching\\n\\n\"\n",
    "\"Important: avoid aggressive rewrites that change meaning.\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "SYNONYMS = {\n",
    "    # conditions\n",
    "    \"hypertension\": [\"high blood pressure\", \"bp\", \"blood pressure\"],\n",
    "    \"high blood pressure\": [\"hypertension\", \"bp\"],\n",
    "    \"cholesterol\": [\"ldl\", \"hdl\", \"blood lipids\"],\n",
    "    \"diabetes\": [\"dm\", \"diabetes mellitus\", \"blood sugar\", \"glucose\"],\n",
    "    \"obesity\": [\"overweight\", \"high bmi\", \"bmi\"],\n",
    "\n",
    "    # nutrients\n",
    "    \"vitamin c\": [\"ascorbic acid\", \"vit c\"],\n",
    "    \"vitamin d\": [\"d3\", \"cholecalciferol\", \"calciferol\"],\n",
    "    \"omega 3\": [\"epa\", \"dha\", \"fish oil\"],\n",
    "    \"fiber\": [\"dietary fiber\", \"roughage\"],\n",
    "    \"protein\": [\"amino acids\", \"lean protein\"],\n",
    "    \"carbohydrates\": [\"carbs\", \"sugars\", \"starch\"],\n",
    "    \"salt\": [\"sodium\", \"na\"],\n",
    "\n",
    "    # diet patterns\n",
    "    \"mediterranean diet\": [\"olive oil diet\", \"med diet\"],\n",
    "    \"low carb\": [\"ketogenic\", \"keto\"],\n",
    "    \"weight loss\": [\"calorie deficit\", \"fat loss\"],\n",
    "}\n",
    "\n",
    "def normalize_query(q: str) -> str:\n",
    "    q = q.strip().lower()\n",
    "    q = re.sub(r\"\\\\s+\", \" \", q)\n",
    "    q = q.replace(\"kilogram\", \"kg\").replace(\"kilograms\", \"kg\")\n",
    "    q = q.replace(\"milligram\", \"mg\").replace(\"milligrams\", \"mg\")\n",
    "    q = q.replace(\"gram\", \"g\").replace(\"grams\", \"g\")\n",
    "    return q\n",
    "\n",
    "def light_spell_fix(q: str) -> str:\n",
    "    if SpellChecker is None:\n",
    "        return q\n",
    "    sp = SpellChecker()\n",
    "    toks = q.split()\n",
    "    fixed = []\n",
    "    for t in toks:\n",
    "        fixed.append(t if len(t) <= 2 else (sp.correction(t) or t))\n",
    "    return \" \".join(fixed)\n",
    "\n",
    "def wordnet_synonyms(term: str, max_syn: int = 2) -> List[str]:\n",
    "    if wn is None:\n",
    "        return []\n",
    "    out = set()\n",
    "    for syn in wn.synsets(term):\n",
    "        for lemma in syn.lemmas():\n",
    "            s = lemma.name().replace(\"_\", \" \").lower()\n",
    "            if s != term.lower():\n",
    "                out.add(s)\n",
    "            if len(out) >= max_syn:\n",
    "                return list(out)\n",
    "    return list(out)\n",
    "\n",
    "def expand_query(q: str, max_alts: int = 4) -> List[str]:\n",
    "    qn = light_spell_fix(normalize_query(q))\n",
    "    alts = [qn]\n",
    "\n",
    "    # dictionary expansions\n",
    "    for key, syns in SYNONYMS.items():\n",
    "        if key in qn:\n",
    "            for s in syns[:2]:\n",
    "                alts.append(qn.replace(key, s))\n",
    "\n",
    "    # WordNet expansion only for short queries\n",
    "    if wn is not None and len(qn.split()) <= 3:\n",
    "        for w in qn.split():\n",
    "            for s in wordnet_synonyms(w, max_syn=1):\n",
    "                alts.append(qn.replace(w, s))\n",
    "\n",
    "    # fuzzy match if user typed close to a key\n",
    "    if fuzz is not None:\n",
    "        best_key, best_score = None, 0\n",
    "        for key in SYNONYMS.keys():\n",
    "            score = fuzz.partial_ratio(qn, key)\n",
    "            if score > best_score:\n",
    "                best_score, best_key = score, key\n",
    "        if best_key and best_score >= 90 and best_key not in qn:\n",
    "            alts.append(qn + \" \" + best_key)\n",
    "\n",
    "    # short focused alternative\n",
    "    short = \" \".join([t for t in qn.split() if len(t) > 2][:6]).strip()\n",
    "    if short and short != qn:\n",
    "        alts.append(short)\n",
    "\n",
    "    # dedupe + cap\n",
    "    uniq, seen = [], set()\n",
    "    for a in alts:\n",
    "        a = a.strip()\n",
    "        if a and a not in seen:\n",
    "            uniq.append(a)\n",
    "            seen.add(a)\n",
    "    return uniq[:max_alts]\n",
    "\n",
    "print(expand_query(\"Hypertensoin vitamin C foods\"))\n",
    "\"\"\"))\n",
    "\n",
    "md(\"## STEP 7 — Retrieval Fusion + Cross‑Encoder Reranking\\n\"\n",
    "\"**Fusion** combines results from multiple query variants.\\n\"\n",
    "\"**Cross‑encoder reranking** improves precision by directly scoring (question, chunk) pairs.\\n\\n\"\n",
    "\"This is usually the biggest retrieval quality boost for RAG.\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker = CrossEncoder(RERANK_MODEL) if CrossEncoder is not None else None\n",
    "\n",
    "def hybrid_retrieve_one_query(q: str, top_k: int = 12, where: Optional[Dict]=None) -> Dict[str, float]:\n",
    "    # returns chunk_id -> score\n",
    "    scores = {}\n",
    "\n",
    "    # embeddings (Chroma if possible)\n",
    "    if chroma_col is not None:\n",
    "        emb_res = retrieve_chroma(q, top_k=top_k, where=where)\n",
    "        for r in emb_res:\n",
    "            scores[r[\"chunk_id\"]] = max(scores.get(r[\"chunk_id\"], 0.0), float(r[\"score\"]))\n",
    "    else:\n",
    "        emb = retrieve_embed_fallback(q, top_k=top_k)\n",
    "        emb_scores = minmax_norm([s for _, s in emb])\n",
    "        for (i, _), sc in zip(emb, emb_scores):\n",
    "            scores[chunks[i].chunk_id] = max(scores.get(chunks[i].chunk_id, 0.0), 0.6 * sc)\n",
    "\n",
    "    # BM25 (in-memory)\n",
    "    bm = retrieve_bm25(q, top_k=top_k)\n",
    "    bm_scores = minmax_norm([s for _, s in bm])\n",
    "    for (i, _), sc in zip(bm, bm_scores):\n",
    "        scores[chunks[i].chunk_id] = max(scores.get(chunks[i].chunk_id, 0.0), 0.4 * sc)\n",
    "\n",
    "    return scores\n",
    "\n",
    "def fusion_retrieve(question: str, fused_topk: int = 20, where: Optional[Dict]=None):\n",
    "    variants = expand_query(question)\n",
    "    fused: Dict[str, float] = {}\n",
    "\n",
    "    for q in variants:\n",
    "        s = hybrid_retrieve_one_query(q, top_k=max(12, fused_topk), where=where)\n",
    "        for cid, sc in s.items():\n",
    "            fused[cid] = max(fused.get(cid, 0.0), sc)\n",
    "\n",
    "    ranked = sorted(fused.items(), key=lambda x: x[1], reverse=True)[:fused_topk]\n",
    "\n",
    "    # Resolve chunk text + metadata\n",
    "    by_id = {c.chunk_id: c for c in chunks}\n",
    "    results = []\n",
    "    for cid, sc in ranked:\n",
    "        c = by_id.get(cid)\n",
    "        if c is None:\n",
    "            continue\n",
    "        results.append({\n",
    "            \"chunk_id\": c.chunk_id,\n",
    "            \"text\": c.text,\n",
    "            \"meta\": c.meta,\n",
    "            \"score\": float(sc),\n",
    "            \"source_file\": c.source_file\n",
    "        })\n",
    "    return results, variants\n",
    "\n",
    "def cross_encoder_rerank(question: str, retrieved: List[Dict], top_n: int = 10) -> List[Dict]:\n",
    "    if reranker is None:\n",
    "        # fallback: no reranking available\n",
    "        return retrieved[:top_n]\n",
    "    pairs = [(question, r[\"text\"]) for r in retrieved]\n",
    "    ce_scores = reranker.predict(pairs)\n",
    "    out = []\n",
    "    for r, s in zip(retrieved, ce_scores):\n",
    "        r2 = dict(r)\n",
    "        r2[\"rerank_score\"] = float(s)\n",
    "        out.append(r2)\n",
    "    out.sort(key=lambda x: x.get(\"rerank_score\", -1e9), reverse=True)\n",
    "    return out[:top_n]\n",
    "\n",
    "# Demo\n",
    "question = \"What foods help with high blood pressure?\"\n",
    "retrieved, variants = fusion_retrieve(question, fused_topk=25, where={\"topic\": {\"$in\": [\"nutrition\",\"food\",\"unknown\"]}} if chroma_col else None)\n",
    "reranked = cross_encoder_rerank(question, retrieved, top_n=10)\n",
    "\n",
    "print(\"Query variants:\", variants)\n",
    "print(\"Top reranked:\")\n",
    "for r in reranked[:3]:\n",
    "    print(\"-\", r[\"chunk_id\"], \"| rerank:\", round(r.get(\"rerank_score\", r[\"score\"]), 3), \"|\", Path(r[\"source_file\"]).name)\n",
    "\"\"\"))\n",
    "\n",
    "md(\"## STEP 8 — Augmentation (Prompt with citations)\\n\"\n",
    "\"We build a strict prompt:\\n\"\n",
    "\"- Answer only from context\\n\"\n",
    "\"- If missing → say \\\"I don't know\\\"\\n\"\n",
    "\"- Always return citations (chunk IDs)\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "def build_context(chunks_list: List[Dict], max_chunks: int = 6, max_chars_each: int = 900) -> str:\n",
    "    ctx = []\n",
    "    for r in chunks_list[:max_chunks]:\n",
    "        ctx.append(f\"[{r['chunk_id']}] (source={Path(r['source_file']).name})\\\\n{r['text'][:max_chars_each]}\")\n",
    "    return \"\\\\n\\\\n\".join(ctx)\n",
    "\n",
    "def build_prompt(question: str, chunks_list: List[Dict]) -> str:\n",
    "    context = build_context(chunks_list)\n",
    "    return f\\\"\\\"\\\"You are a helpful assistant.\n",
    "Answer the question ONLY using the provided context.\n",
    "If the context does not contain the answer, say: \"I don't know from the provided documents.\"\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Rules:\n",
    "- Use ONLY context facts.\n",
    "- End with: Citations: [chunk_id1, chunk_id2, ...]\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "prompt = build_prompt(question, reranked)\n",
    "print(prompt[:900], \"...\")\n",
    "\"\"\"))\n",
    "\n",
    "md(\"## STEP 9 — Generation + Self‑Correction Verifier\\n\"\n",
    "\"**Generation** produces the initial answer.\\n\"\n",
    "\"**Verifier pass** removes unsupported claims and enforces citations.\\n\\n\"\n",
    "\"In this notebook we include a safe **stub generator** so it runs everywhere.\\n\"\n",
    "\"Replace `generator()` with your real model/API call when you are ready.\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "def generator_stub(prompt: str) -> str:\n",
    "    # Replace this with OpenAI / local model call.\n",
    "    # Keep it deterministic for reproducible evaluation.\n",
    "    return (\n",
    "        \"I don't know from the provided documents.\\\\n\"\n",
    "        \"Citations: []\"\n",
    "    )\n",
    "\n",
    "def generate_answer(question: str, chunks_list: List[Dict]) -> Tuple[str, str]:\n",
    "    prompt = build_prompt(question, chunks_list)\n",
    "    draft = generator_stub(prompt)\n",
    "    return draft, prompt\n",
    "\n",
    "def verifier_pass(question: str, draft: str, chunks_list: List[Dict]) -> str:\n",
    "    # If you have a real LLM, use it here.\n",
    "    # This stub enforces a rule: if draft has no citations, force 'I don't know'.\n",
    "    if \"Citations\" not in draft:\n",
    "        return 'I don\\\\'t know from the provided documents.\\\\nCitations: []'\n",
    "    return draft\n",
    "\n",
    "draft_answer, used_prompt = generate_answer(question, reranked)\n",
    "final_answer = verifier_pass(question, draft_answer, reranked)\n",
    "\n",
    "print(\"Draft:\", draft_answer)\n",
    "print(\"\\\\nFinal:\", final_answer)\n",
    "\"\"\"))\n",
    "\n",
    "md(\"## STEP 10 — Evaluation (Retrieval quality)\\n\"\n",
    "\"We evaluate retrieval using:\\n\"\n",
    "\"- **Hit@K** (did we retrieve at least one relevant chunk?)\\n\"\n",
    "\"- **MRR@K** (how early did the first relevant chunk appear?)\\n\\n\"\n",
    "\"Create `eval_questions.jsonl` lines like:\\n\"\n",
    "\"`{\\\"query\\\": \\\"...\\\", \\\"relevant_chunk_ids\\\": [\\\"...\\\"]}`\\n\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "def hit_at_k(ranked_ids: List[str], relevant: set, k: int) -> float:\n",
    "    return 1.0 if any(cid in relevant for cid in ranked_ids[:k]) else 0.0\n",
    "\n",
    "def mrr_at_k(ranked_ids: List[str], relevant: set, k: int) -> float:\n",
    "    for i, cid in enumerate(ranked_ids[:k], start=1):\n",
    "        if cid in relevant:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_retrieval(eval_path: str = \"eval_questions.jsonl\", ks=(1,3,5,10)):\n",
    "    p = Path(eval_path)\n",
    "    if not p.exists():\n",
    "        print(f\"❌ Missing {eval_path}. Create it first.\")\n",
    "        return\n",
    "\n",
    "    rows = [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    if not rows:\n",
    "        print(\"❌ No evaluation rows found.\")\n",
    "        return\n",
    "\n",
    "    stats = {k: {\"hit\": [], \"mrr\": []} for k in ks}\n",
    "\n",
    "    for r in rows:\n",
    "        q = r[\"query\"]\n",
    "        relevant = set(r[\"relevant_chunk_ids\"])\n",
    "\n",
    "        retrieved, _ = fusion_retrieve(q, fused_topk=max(ks))\n",
    "        reranked = cross_encoder_rerank(q, retrieved, top_n=max(ks))\n",
    "        ranked_ids = [x[\"chunk_id\"] for x in reranked]\n",
    "\n",
    "        for k in ks:\n",
    "            stats[k][\"hit\"].append(hit_at_k(ranked_ids, relevant, k))\n",
    "            stats[k][\"mrr\"].append(mrr_at_k(ranked_ids, relevant, k))\n",
    "\n",
    "    print(\"=== Retrieval Evaluation ===\")\n",
    "    for k in ks:\n",
    "        print(f\"Hit@{k}: {float(np.mean(stats[k]['hit'])):.3f} | MRR@{k}: {float(np.mean(stats[k]['mrr'])):.3f}\")\n",
    "\n",
    "# evaluate_retrieval(\"eval_questions.jsonl\")\n",
    "\"\"\"))\n",
    "\n",
    "md(\"## STEP 11 — Improved Notebook UI (filters + citations + expandable chunks)\\n\"\n",
    "\"This UI is designed for a demo:\\n\"\n",
    "\"- Query box\\n\"\n",
    "\"- Topic/language filters (metadata)\\n\"\n",
    "\"- TopK control\\n\"\n",
    "\"- Retrieved chunks shown in expandable accordions\\n\"\n",
    "\"- Prompt preview\\n\"\n",
    "\"- Answer + citations area\")\n",
    "\n",
    "code(textwrap.dedent(\"\"\"\n",
    "if widgets is None:\n",
    "    print(\"❌ ipywidgets not installed. Install it in STEP 0.\")\n",
    "else:\n",
    "    q_box = widgets.Text(value=\"foods for high blood pressure\", description=\"Query:\", layout=widgets.Layout(width=\"55%\"))\n",
    "    topk = widgets.IntSlider(value=10, min=5, max=20, step=1, description=\"TopK:\", continuous_update=False)\n",
    "\n",
    "    topic_dd = widgets.Dropdown(\n",
    "        options=[\"any\", \"nutrition\", \"food\", \"unknown\"],\n",
    "        value=\"any\",\n",
    "        description=\"Topic:\"\n",
    "    )\n",
    "    lang_dd = widgets.Dropdown(\n",
    "        options=[\"any\", \"en\", \"he\"],\n",
    "        value=\"any\",\n",
    "        description=\"Lang:\"\n",
    "    )\n",
    "\n",
    "    btn = widgets.Button(description=\"Search\", button_style=\"primary\")\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def make_where(topic_val: str, lang_val: str):\n",
    "        if chroma_col is None:\n",
    "            return None\n",
    "        where = {}\n",
    "        if topic_val != \"any\":\n",
    "            where[\"topic\"] = topic_val\n",
    "        if lang_val != \"any\":\n",
    "            where[\"language\"] = lang_val\n",
    "        return where if where else None\n",
    "\n",
    "    def on_search(_):\n",
    "        with out:\n",
    "            clear_output()\n",
    "            question = q_box.value.strip()\n",
    "            if not question:\n",
    "                display(Markdown(\"**Please enter a query.**\"))\n",
    "                return\n",
    "\n",
    "            where = make_where(topic_dd.value, lang_dd.value)\n",
    "            retrieved, variants = fusion_retrieve(question, fused_topk=max(25, topk.value), where=where)\n",
    "            reranked = cross_encoder_rerank(question, retrieved, top_n=topk.value)\n",
    "\n",
    "            display(Markdown(f\"### Query variants\\n`{variants}`\"))\n",
    "            display(Markdown(\"### Top reranked chunks (click to expand)\"))\n",
    "\n",
    "            # Accordion of chunks\n",
    "            items = []\n",
    "            titles = []\n",
    "            for i, r in enumerate(reranked, start=1):\n",
    "                header = f\"#{i}  rerank={r.get('rerank_score', r['score']):.3f}  |  {Path(r['source_file']).name}  |  {r['chunk_id']}\"\n",
    "                titles.append(header)\n",
    "                items.append(widgets.Textarea(value=r[\"text\"], layout=widgets.Layout(width=\"100%\", height=\"130px\")))\n",
    "\n",
    "            acc = widgets.Accordion(children=items)\n",
    "            for i, t in enumerate(titles):\n",
    "                acc.set_title(i, t)\n",
    "            display(acc)\n",
    "\n",
    "            prompt = build_prompt(question, reranked)\n",
    "            display(Markdown(\"### Prompt preview\"))\n",
    "            display(widgets.Textarea(value=prompt[:2500], layout=widgets.Layout(width=\"100%\", height=\"220px\")))\n",
    "\n",
    "            draft, _ = generate_answer(question, reranked)\n",
    "            final = verifier_pass(question, draft, reranked)\n",
    "\n",
    "            display(Markdown(\"### Final answer (after verifier)\"))\n",
    "            display(widgets.Textarea(value=final, layout=widgets.Layout(width=\"100%\", height=\"120px\")))\n",
    "\n",
    "    btn.on_click(on_search)\n",
    "    display(widgets.VBox([widgets.HBox([q_box, topk, topic_dd, lang_dd, btn]), out]))\n",
    "\"\"\"))\n",
    "\n",
    "out_path = Path(\"/mnt/data/Rag_Ver3_FoodRAG_Rerank_Verify_Chroma_UI.ipynb\")\n",
    "out_path.write_text(json.dumps(nb, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "str(out_path)\n",
    "\n"
   ],
   "id": "6259a0f976f25096",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\mnt\\\\data\\\\Rag_Ver3_FoodRAG_Rerank_Verify_Chroma_UI.ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 780\u001B[0m\n\u001B[0;32m    706\u001B[0m code(textwrap\u001B[38;5;241m.\u001B[39mdedent(\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    707\u001B[0m \u001B[38;5;124mif widgets is None:\u001B[39m\n\u001B[0;32m    708\u001B[0m \u001B[38;5;124m    print(\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m❌ ipywidgets not installed. Install it in STEP 0.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    776\u001B[0m \u001B[38;5;124m    display(widgets.VBox([widgets.HBox([q_box, topk, topic_dd, lang_dd, btn]), out]))\u001B[39m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m))\n\u001B[0;32m    779\u001B[0m out_path \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/data/Rag_Ver3_FoodRAG_Rerank_Verify_Chroma_UI.ipynb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 780\u001B[0m out_path\u001B[38;5;241m.\u001B[39mwrite_text(json\u001B[38;5;241m.\u001B[39mdumps(nb, ensure_ascii\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, indent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m), encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28mstr\u001B[39m(out_path)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\pathlib.py:1047\u001B[0m, in \u001B[0;36mPath.write_text\u001B[1;34m(self, data, encoding, errors, newline)\u001B[0m\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata must be str, not \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1045\u001B[0m                     data\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m   1046\u001B[0m encoding \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mtext_encoding(encoding)\n\u001B[1;32m-> 1047\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopen(mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39merrors, newline\u001B[38;5;241m=\u001B[39mnewline) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m   1048\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f\u001B[38;5;241m.\u001B[39mwrite(data)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\pathlib.py:1013\u001B[0m, in \u001B[0;36mPath.open\u001B[1;34m(self, mode, buffering, encoding, errors, newline)\u001B[0m\n\u001B[0;32m   1011\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1012\u001B[0m     encoding \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mtext_encoding(encoding)\n\u001B[1;32m-> 1013\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io\u001B[38;5;241m.\u001B[39mopen(\u001B[38;5;28mself\u001B[39m, mode, buffering, encoding, errors, newline)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '\\\\mnt\\\\data\\\\Rag_Ver3_FoodRAG_Rerank_Verify_Chroma_UI.ipynb'"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0280f9631aaa48af97f21d0770703043": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e9b6595383c743848414044b0ed6b4b8",
       "IPY_MODEL_57b7cb94f85a4bfab5aaeda7af3193bf",
       "IPY_MODEL_d8b8046e415a4bcd994ee22aa53f409a"
      ],
      "layout": "IPY_MODEL_5e25e8bbcf4c44ea9213c08689f47f0f"
     }
    },
    "e9b6595383c743848414044b0ed6b4b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_523249dc955a4f2a90673005bb1ead0c",
      "placeholder": "​",
      "style": "IPY_MODEL_e81ae0d053954deaabbc023225936e4a",
      "value": "Batches: 100%"
     }
    },
    "57b7cb94f85a4bfab5aaeda7af3193bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_240c291eeb4245b7b61e0fc309c4b9d6",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_39d82cf6bc734d8796a9b77f45663781",
      "value": 26
     }
    },
    "d8b8046e415a4bcd994ee22aa53f409a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a949d669e68e4e98bc7cfe61adde0b37",
      "placeholder": "​",
      "style": "IPY_MODEL_fbfb0a2f40914ef2a6a7c3caf634cb3a",
      "value": " 26/26 [00:06&lt;00:00,  3.08it/s]"
     }
    },
    "5e25e8bbcf4c44ea9213c08689f47f0f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "523249dc955a4f2a90673005bb1ead0c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e81ae0d053954deaabbc023225936e4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "240c291eeb4245b7b61e0fc309c4b9d6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39d82cf6bc734d8796a9b77f45663781": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a949d669e68e4e98bc7cfe61adde0b37": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbfb0a2f40914ef2a6a7c3caf634cb3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
