{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reychely/food-rag-web/blob/main/IR_Project_RAG_APP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 0: Settings + Paths (Drive or Local)\n",
        "# What: define project folders for data + storage\n",
        "# Why: keeps your indexed data + outputs persistent\n",
        "# ============================================================\n",
        "\n",
        "import os, json, re, hashlib, math, time\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# If you are in Colab, keep using your Drive path\n",
        "BASE = \"/content/drive/MyDrive/IR_RAG_App\" if IN_COLAB else str(Path.cwd() / \"IR_RAG_App\")\n",
        "DATA_DIR = Path(BASE) / \"data\" / \"raw\"\n",
        "STORAGE_DIR = Path(BASE) / \"storage\"\n",
        "\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "STORAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ IN_COLAB:\", IN_COLAB)\n",
        "print(\"‚úÖ DATA_DIR:\", DATA_DIR)\n",
        "print(\"‚úÖ STORAGE_DIR:\", STORAGE_DIR)\n",
        "\n",
        "# List files\n",
        "files = sorted([p for p in DATA_DIR.rglob(\"*\") if p.is_file()])\n",
        "print(f\"üìÑ Files found: {len(files)}\")\n",
        "for p in files[:15]:\n",
        "    print(\" -\", p.name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDwF7qRXaZAl",
        "outputId": "c1cf87a3-2328-4a4c-cfab-89f3e6f162bf"
      },
      "id": "rDwF7qRXaZAl",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ IN_COLAB: True\n",
            "‚úÖ DATA_DIR: /content/drive/MyDrive/IR_RAG_App/data/raw\n",
            "‚úÖ STORAGE_DIR: /content/drive/MyDrive/IR_RAG_App/storage\n",
            "üìÑ Files found: 59\n",
            " - Animal_nutrition.txt\n",
            " - CRON-diet.txt\n",
            " - Calorie_restriction.txt\n",
            " - Child_Nutrition_Act.txt\n",
            " - Cottage_cheese.txt\n",
            " - Dal.txt\n",
            " - Diet_(nutrition).txt\n",
            " - Dietary_Guidelines_for_Americans.txt\n",
            " - Dietary_Guidelines_for_Americans_2020-2025.pdf\n",
            " - Eating a balanced diet - NHS.html\n",
            " - Eating.txt\n",
            " - Empty_calories.txt\n",
            " - Equine_nutrition.txt\n",
            " - Essential_amino_acid.txt\n",
            " - Flavonoid.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 1: Install dependencies (fast, only if missing)\n",
        "# What: install required libs for RAG pipeline\n",
        "# Why: avoids slow re-install each run\n",
        "# ============================================================\n",
        "\n",
        "import importlib, sys, subprocess\n",
        "\n",
        "def pip_install_if_missing(pkgs):\n",
        "    missing = []\n",
        "    for mod, pip_name in pkgs:\n",
        "        try:\n",
        "            importlib.import_module(mod)\n",
        "        except:\n",
        "            missing.append(pip_name)\n",
        "    if missing:\n",
        "        print(\"üì¶ Installing:\", missing)\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + missing)\n",
        "        print(\"‚úÖ Installed missing packages\")\n",
        "    else:\n",
        "        print(\"‚úÖ All dependencies already installed\")\n",
        "\n",
        "pip_install_if_missing([\n",
        "    (\"numpy\", \"numpy\"),\n",
        "    (\"tqdm\", \"tqdm\"),\n",
        "    (\"pypdf\", \"pypdf\"),\n",
        "    (\"docx\", \"python-docx\"),\n",
        "    (\"bs4\", \"beautifulsoup4\"),\n",
        "    (\"requests\", \"requests\"),\n",
        "    (\"sentence_transformers\", \"sentence-transformers\"),\n",
        "    (\"faiss\", \"faiss-cpu\"),\n",
        "    (\"rank_bm25\", \"rank_bm25\"),\n",
        "    (\"symspellpy\", \"symspellpy\"),\n",
        "    (\"nltk\", \"nltk\"),\n",
        "    (\"transformers\", \"transformers\"),\n",
        "    (\"torch\", \"torch\"),\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtI0Myr1ar0R",
        "outputId": "ad102f71-a135-459d-dc2c-382449cb0ae2"
      },
      "id": "NtI0Myr1ar0R",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All dependencies already installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 2: Imports + NLTK resources\n",
        "# What: init NLP helpers (lemmatizer, wordnet)\n",
        "# Why: better lexical retrieval + safer query expansion\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "nltk.download(\"omw-1.4\", quiet=True)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"‚úÖ NLTK ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70gWSAqWa7RL",
        "outputId": "faafdb88-d2e9-4364-cd1e-4d69b47eebff"
      },
      "id": "70gWSAqWa7RL",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ NLTK ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 3: Text cleaning + normalization\n",
        "# What: consistent cleaning for docs + queries\n",
        "# Why: improves BM25 + reduces noise in embeddings\n",
        "# ============================================================\n",
        "\n",
        "_token_re = re.compile(r\"[a-zA-Z]{2,}\")\n",
        "\n",
        "def normalize_for_bm25(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # keep english letters only\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def clean_text_general(text: str) -> str:\n",
        "    # remove obvious boilerplate/noise + normalize spaces\n",
        "    text = text.replace(\"\\x00\", \" \")\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "yO482Ojua4is"
      },
      "id": "yO482Ojua4is",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 4: Load documents (txt/pdf/docx/html)\n",
        "# What: read multiple formats into a unified list\n",
        "# Why: supports expanding your corpus beyond .txt\n",
        "# ============================================================\n",
        "\n",
        "from typing import List, Dict\n",
        "from pypdf import PdfReader\n",
        "import docx\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def load_txt(path: Path) -> str:\n",
        "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "def load_pdf(path: Path) -> str:\n",
        "    reader = PdfReader(str(path))\n",
        "    pages = []\n",
        "    for pg in reader.pages:\n",
        "        pages.append(pg.extract_text() or \"\")\n",
        "    return \"\\n\".join(pages)\n",
        "\n",
        "def load_docx(path: Path) -> str:\n",
        "    d = docx.Document(str(path))\n",
        "    return \"\\n\".join([p.text for p in d.paragraphs])\n",
        "\n",
        "def load_html(path: Path) -> str:\n",
        "    raw = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    soup = BeautifulSoup(raw, \"html.parser\")\n",
        "    # drop scripts/styles\n",
        "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
        "        tag.decompose()\n",
        "    return soup.get_text(\" \", strip=True)\n",
        "\n",
        "LOADERS = {\n",
        "    \".txt\": load_txt,\n",
        "    \".pdf\": load_pdf,\n",
        "    \".docx\": load_docx,\n",
        "    \".html\": load_html,\n",
        "    \".htm\": load_html,\n",
        "}\n",
        "\n",
        "def load_one_file(p: Path) -> str:\n",
        "    ext = p.suffix.lower()\n",
        "    if ext not in LOADERS:\n",
        "        return \"\"\n",
        "    try:\n",
        "        t = LOADERS[ext](p)\n",
        "        return clean_text_general(t)\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Failed:\", p.name, \"|\", e)\n",
        "        return \"\"\n",
        "\n",
        "def load_corpus(data_dir: Path, min_chars=200) -> List[Dict]:\n",
        "    docs = []\n",
        "    for p in sorted([x for x in data_dir.rglob(\"*\") if x.is_file()]):\n",
        "        text = load_one_file(p)\n",
        "        # filter tiny/noisy docs\n",
        "        if len(text) < min_chars:\n",
        "            continue\n",
        "        docs.append({\n",
        "            \"source\": p.name,\n",
        "            \"path\": str(p),\n",
        "            \"ext\": p.suffix.lower(),\n",
        "            \"text\": text\n",
        "        })\n",
        "    return docs\n",
        "\n",
        "docs = load_corpus(DATA_DIR, min_chars=200)\n",
        "print(f\"‚úÖ Loaded {len(docs)} docs (min_chars=200)\")\n",
        "if docs:\n",
        "    print(\"Example:\", docs[0][\"source\"], \"| chars:\", len(docs[0][\"text\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GhNqadYbu4S",
        "outputId": "450676fc-5a05-4922-e62d-47e027b2bcd1"
      },
      "id": "7GhNqadYbu4S",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 58 docs (min_chars=200)\n",
            "Example: Animal_nutrition.txt | chars: 6142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 5: Chunking + Dedup (exact duplicates)\n",
        "# What: split docs into overlapping chunks; remove duplicates\n",
        "# Why: chunking improves retrieval, dedup avoids repetitive answers\n",
        "# ============================================================\n",
        "\n",
        "CHUNK_SIZE = 900\n",
        "CHUNK_OVERLAP = 150\n",
        "\n",
        "def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
        "    chunks = []\n",
        "    n = len(text)\n",
        "    step = max(1, chunk_size - overlap)\n",
        "    start = 0\n",
        "    while start < n:\n",
        "        end = min(start + chunk_size, n)\n",
        "        c = text[start:end].strip()\n",
        "        if c:\n",
        "            chunks.append(c)\n",
        "        start += step\n",
        "    return chunks\n",
        "\n",
        "def sha1(s: str) -> str:\n",
        "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "\n",
        "chunks = []\n",
        "seen_hash = set()\n",
        "\n",
        "for d in docs:\n",
        "    parts = chunk_text(d[\"text\"])\n",
        "    for i, c in enumerate(parts):\n",
        "        h = sha1(c)\n",
        "        if h in seen_hash:\n",
        "            continue\n",
        "        seen_hash.add(h)\n",
        "        chunks.append({\n",
        "            \"source\": d[\"source\"],\n",
        "            \"path\": d[\"path\"],\n",
        "            \"ext\": d[\"ext\"],\n",
        "            \"chunk_id\": f\"{d['source']}::chunk{i}\",\n",
        "            \"text\": c\n",
        "        })\n",
        "\n",
        "print(f\"‚úÖ Created {len(chunks)} unique chunks from {len(docs)} docs\")\n",
        "print(\"Preview:\", chunks[0][\"chunk_id\"], \"|\", chunks[0][\"text\"][:160], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbBn238cb8K3",
        "outputId": "30266500-286b-4ef2-8c55-b86a1fee7ba7"
      },
      "id": "IbBn238cb8K3",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created 1637 unique chunks from 58 docs\n",
            "Preview: Animal_nutrition.txt::chunk0 | Animal nutrition focuses on the dietary nutrients needs of animals, primarily those in agriculture and food production, but also in zoos, aquariums, and wildlif ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 6: Embeddings + FAISS index (semantic search)\n",
        "# What: embed chunks + build FAISS vector index\n",
        "# Why: handles paraphrases + semantic similarity\n",
        "# ============================================================\n",
        "\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(EMB_MODEL)\n",
        "\n",
        "texts = [c[\"text\"] for c in chunks]\n",
        "emb = embedder.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
        "emb = np.asarray(emb, dtype=\"float32\")\n",
        "\n",
        "dim = emb.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)  # cosine similarity because normalized\n",
        "index.add(emb)\n",
        "\n",
        "faiss_path = STORAGE_DIR / \"faiss.index\"\n",
        "meta_path = STORAGE_DIR / \"chunks_meta.json\"\n",
        "\n",
        "faiss.write_index(index, str(faiss_path))\n",
        "meta_path.write_text(json.dumps(chunks, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "print(\"‚úÖ Embeddings shape:\", emb.shape)\n",
        "print(\"‚úÖ FAISS index size:\", index.ntotal)\n",
        "print(\"‚úÖ Saved:\", faiss_path)\n",
        "print(\"‚úÖ Saved:\", meta_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "0280f9631aaa48af97f21d0770703043",
            "e9b6595383c743848414044b0ed6b4b8",
            "57b7cb94f85a4bfab5aaeda7af3193bf",
            "d8b8046e415a4bcd994ee22aa53f409a",
            "5e25e8bbcf4c44ea9213c08689f47f0f",
            "523249dc955a4f2a90673005bb1ead0c",
            "e81ae0d053954deaabbc023225936e4a",
            "240c291eeb4245b7b61e0fc309c4b9d6",
            "39d82cf6bc734d8796a9b77f45663781",
            "a949d669e68e4e98bc7cfe61adde0b37",
            "fbfb0a2f40914ef2a6a7c3caf634cb3a"
          ]
        },
        "id": "vEzIYyRGb_PX",
        "outputId": "eafbdd82-9316-4eb7-e8a8-3ae7037c2eed"
      },
      "id": "vEzIYyRGb_PX",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0280f9631aaa48af97f21d0770703043"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Embeddings shape: (1637, 384)\n",
            "‚úÖ FAISS index size: 1637\n",
            "‚úÖ Saved: /content/drive/MyDrive/IR_RAG_App/storage/faiss.index\n",
            "‚úÖ Saved: /content/drive/MyDrive/IR_RAG_App/storage/chunks_meta.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 7: BM25 index (lexical search) + vocab frequencies\n",
        "# What: tokenize (lemmatize) + build BM25; build vocab freq\n",
        "# Why: BM25 helps exact matches (ingredients, terms, acronyms)\n",
        "# ============================================================\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize_lemma(text: str) -> List[str]:\n",
        "    t = normalize_for_bm25(text)\n",
        "    toks = _token_re.findall(t)\n",
        "    toks = [lemmatizer.lemmatize(x) for x in toks]\n",
        "    return toks\n",
        "\n",
        "bm25_tokens = [tokenize_lemma(c[\"text\"]) for c in tqdm(chunks, desc=\"BM25 tokenize\")]\n",
        "bm25 = BM25Okapi(bm25_tokens)\n",
        "\n",
        "bm25_vocab = Counter()\n",
        "for toks in bm25_tokens:\n",
        "    bm25_vocab.update(toks)\n",
        "\n",
        "print(\"‚úÖ BM25 ready | vocab size:\", len(bm25_vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5StGE-mScBk1",
        "outputId": "d9b9d0e8-8662-48a5-c359-e3a8caf526e9"
      },
      "id": "5StGE-mScBk1",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BM25 tokenize: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1637/1637 [00:02<00:00, 668.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ BM25 ready | vocab size: 10727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 8: Spell correction (SymSpell) + safe overrides\n",
        "# What: correct typos without semantic drift (is->his, to->two)\n",
        "# Why: user queries are messy; safer correction improves retrieval\n",
        "# ============================================================\n",
        "\n",
        "# download common English words (50k)\n",
        "import urllib.request\n",
        "vocab_file = STORAGE_DIR / \"en_vocab.txt\"\n",
        "if not vocab_file.exists():\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2018/en/en_50k.txt\",\n",
        "        str(vocab_file)\n",
        "    )\n",
        "print(\"‚úÖ en_vocab.txt:\", vocab_file)\n",
        "\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "# stopwords we never correct\n",
        "STOPWORDS = {\n",
        "    \"what\",\"why\",\"how\",\"which\",\"when\",\"where\",\"who\",\n",
        "    \"is\",\"are\",\"was\",\"were\",\"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\n",
        "    \"and\",\"or\",\"not\",\"a\",\"an\",\"the\",\"this\",\"that\",\"it\"\n",
        "}\n",
        "\n",
        "# common typo overrides (nutrition-friendly)\n",
        "COMMON_TYPO_OVERRIDES = {\n",
        "    \"wht\": \"what\",\n",
        "    \"wats\": \"what\",\n",
        "    \"whats\": \"what\",\n",
        "    \"healty\": \"healthy\",\n",
        "    \"helthy\": \"healthy\",\n",
        "    \"unhealty\": \"unhealthy\",\n",
        "    \"calory\": \"calorie\",\n",
        "}\n",
        "\n",
        "def edit_distance(a: str, b: str) -> int:\n",
        "    m, n = len(a), len(b)\n",
        "    dp = list(range(n + 1))\n",
        "    for i in range(1, m + 1):\n",
        "        prev = dp[0]\n",
        "        dp[0] = i\n",
        "        for j in range(1, n + 1):\n",
        "            cur = dp[j]\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[j] = min(dp[j] + 1, dp[j-1] + 1, prev + cost)\n",
        "            prev = cur\n",
        "    return dp[n]\n",
        "\n",
        "# init symspell\n",
        "symspell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "\n",
        "# load vocab sets\n",
        "EN_VOCAB_SET = set()\n",
        "with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        w = line.strip().lower()\n",
        "        if len(w) >= 2:\n",
        "            EN_VOCAB_SET.add(w)\n",
        "            symspell.create_dictionary_entry(w, 1000)\n",
        "\n",
        "DOMAIN_VOCAB_SET = set(bm25_vocab.keys())\n",
        "for term, freq in bm25_vocab.items():\n",
        "    if len(term) >= 2:\n",
        "        symspell.create_dictionary_entry(term, int(freq))\n",
        "\n",
        "KNOWN_WORDS = EN_VOCAB_SET | DOMAIN_VOCAB_SET\n",
        "\n",
        "def symspell_fix_query(q: str) -> str:\n",
        "    qn = normalize_for_bm25(q)\n",
        "    out = []\n",
        "\n",
        "    for tok in qn.split():\n",
        "        if tok in COMMON_TYPO_OVERRIDES:\n",
        "            out.append(COMMON_TYPO_OVERRIDES[tok])\n",
        "            continue\n",
        "        if tok in STOPWORDS or len(tok) <= 2:\n",
        "            out.append(tok)\n",
        "            continue\n",
        "        if tok in KNOWN_WORDS:\n",
        "            out.append(tok)\n",
        "            continue\n",
        "\n",
        "        sugg = symspell.lookup(tok, Verbosity.TOP, max_edit_distance=2)\n",
        "        if not sugg:\n",
        "            out.append(tok); continue\n",
        "\n",
        "        best = sugg[0].term\n",
        "        if best not in KNOWN_WORDS:\n",
        "            out.append(tok); continue\n",
        "        if edit_distance(tok, best) > 2:\n",
        "            out.append(tok); continue\n",
        "\n",
        "        # block known bad swaps\n",
        "        BAD_SWAPS = {(\"wht\",\"who\"), (\"to\",\"two\"), (\"is\",\"his\")}\n",
        "        if (tok, best) in BAD_SWAPS:\n",
        "            out.append(tok); continue\n",
        "\n",
        "        out.append(best)\n",
        "\n",
        "    return \" \".join(out)\n",
        "\n",
        "# sanity\n",
        "print(\"TEST1:\", symspell_fix_query(\"Why is junk food considered unhealhy?\"))\n",
        "print(\"TEST2:\", symspell_fix_query(\"Wht is healty to eat?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cV3TXWZcFWc",
        "outputId": "1c292c7e-494a-4c3f-e0e9-462d1c50c7d6"
      },
      "id": "8cV3TXWZcFWc",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ en_vocab.txt: /content/drive/MyDrive/IR_RAG_App/storage/en_vocab.txt\n",
            "TEST1: why is junk food considered unhealthy\n",
            "TEST2: what is healthy to eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 9: Query expansion (lightweight + domain hints)\n",
        "# What: add a few related terms (synonyms + nutrition hints)\n",
        "# Why: helps when docs use different wording than query\n",
        "# ============================================================\n",
        "\n",
        "DOMAIN_HINTS = {\n",
        "    \"healthy\": [\"balanced\", \"nutrient-dense\", \"whole foods\", \"vegetables\", \"fruits\", \"fiber\"],\n",
        "    \"unhealthy\": [\"junk\", \"processed\", \"ultra-processed\", \"high sugar\", \"high salt\", \"high fat\"],\n",
        "    \"junk\": [\"fast food\", \"ultra-processed\", \"snacks\", \"sugary drinks\"],\n",
        "    \"calories\": [\"kcal\", \"kilocalories\", \"food energy\"],\n",
        "    \"nutrients\": [\"vitamins\", \"minerals\", \"macronutrients\", \"micronutrients\"],\n",
        "}\n",
        "\n",
        "def _wn_synonyms(word: str, max_syn=1):\n",
        "    out = []\n",
        "    for s in wn.synsets(word):\n",
        "        for l in s.lemmas():\n",
        "            w = l.name().replace(\"_\",\" \").lower()\n",
        "            if w != word and w.isalpha() and w not in out:\n",
        "                out.append(w)\n",
        "            if len(out) >= max_syn:\n",
        "                return out\n",
        "    return out\n",
        "\n",
        "def expand_query_domain(q: str) -> str:\n",
        "    base = normalize_for_bm25(q)\n",
        "    toks = base.split()\n",
        "\n",
        "    extra = []\n",
        "    for t in toks:\n",
        "        if t not in STOPWORDS and len(t) >= 4:\n",
        "            extra.extend(_wn_synonyms(t, max_syn=1))\n",
        "\n",
        "    for key, adds in DOMAIN_HINTS.items():\n",
        "        if key in toks:\n",
        "            extra.extend(adds)\n",
        "\n",
        "    seen = set(toks)\n",
        "    final_extra = []\n",
        "    for e in extra:\n",
        "        e = normalize_for_bm25(e)\n",
        "        if not e or e in seen:\n",
        "            continue\n",
        "        seen.add(e)\n",
        "        final_extra.append(e)\n",
        "        if len(final_extra) >= 12:\n",
        "            break\n",
        "\n",
        "    return base + (\" \" + \" \".join(final_extra) if final_extra else \"\")\n"
      ],
      "metadata": {
        "id": "SJFtCrV2cGrW"
      },
      "id": "SJFtCrV2cGrW",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 10: Query preparation (typo fix -> rewrite -> expand)\n",
        "# What: builds retrieval query + debug info\n",
        "# Why: prevents NameError and ensures consistent pipeline\n",
        "# ============================================================\n",
        "\n",
        "def prepare_query(q: str) -> dict:\n",
        "    original = q\n",
        "    typo_fixed = symspell_fix_query(q)\n",
        "\n",
        "    rewritten = typo_fixed.strip()\n",
        "    if not rewritten.endswith(\"?\"):\n",
        "        rewritten += \"?\"\n",
        "\n",
        "    expanded = expand_query_domain(typo_fixed)\n",
        "\n",
        "    return {\n",
        "        \"original\": original,\n",
        "        \"typo_fixed\": typo_fixed,\n",
        "        \"rewritten\": rewritten,\n",
        "        \"expanded\": expanded\n",
        "    }\n"
      ],
      "metadata": {
        "id": "gktx_zT8cKR5"
      },
      "id": "gktx_zT8cKR5",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 11: Hybrid retrieval (FAISS + BM25) + dedupe candidates\n",
        "# What: merge semantic and lexical scores\n",
        "# Why: best of both worlds + fewer missed matches\n",
        "# ============================================================\n",
        "\n",
        "def faiss_search(query: str, k=60):\n",
        "    qemb = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
        "    D, I = index.search(qemb, k)\n",
        "    out = []\n",
        "    for score, idx_ in zip(D[0], I[0]):\n",
        "        out.append((int(idx_), float(score)))\n",
        "    return out\n",
        "\n",
        "def bm25_search(query: str, k=60):\n",
        "    toks = tokenize_lemma(query)\n",
        "    scores = bm25.get_scores(toks)\n",
        "    top_idx = np.argsort(scores)[::-1][:k]\n",
        "    return [(int(i), float(scores[i])) for i in top_idx]\n",
        "\n",
        "def retrieve_hybrid_candidates(query: str, faiss_k=60, bm25_k=60, alpha=0.6):\n",
        "    fa = faiss_search(query, k=faiss_k)\n",
        "    bm = bm25_search(query, k=bm25_k)\n",
        "\n",
        "    # normalize bm25 to [0,1] roughly\n",
        "    bm_scores = np.array([s for _, s in bm], dtype=\"float32\")\n",
        "    bm_max = float(bm_scores.max()) if len(bm_scores) else 1.0\n",
        "\n",
        "    merged = {}\n",
        "    for idx_, s in fa:\n",
        "        merged[idx_] = merged.get(idx_, 0.0) + alpha * s\n",
        "    for idx_, s in bm:\n",
        "        merged[idx_] = merged.get(idx_, 0.0) + (1 - alpha) * (s / bm_max if bm_max > 0 else 0.0)\n",
        "\n",
        "    # build candidate list\n",
        "    cands = []\n",
        "    for idx_, score in merged.items():\n",
        "        d = dict(chunks[idx_])\n",
        "        d[\"hybrid_score\"] = float(score)\n",
        "        cands.append(d)\n",
        "\n",
        "    cands.sort(key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
        "    return cands\n"
      ],
      "metadata": {
        "id": "EU6SIZ7mcMRF"
      },
      "id": "EU6SIZ7mcMRF",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 12: Reranker (cross-encoder) for better precision\n",
        "# What: rerank candidates using query+chunk pair scoring\n",
        "# Why: improves relevance, reduces noisy contexts\n",
        "# ============================================================\n",
        "\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker = CrossEncoder(RERANK_MODEL)\n",
        "\n",
        "def rerank_top(query: str, cands: List[Dict], limit=120):\n",
        "    cands = cands[:limit]\n",
        "    pairs = [(query, c[\"text\"]) for c in cands]\n",
        "    scores = reranker.predict(pairs, batch_size=32, show_progress_bar=False)\n",
        "    for c, s in zip(cands, scores):\n",
        "        c[\"rerank_score\"] = float(s)\n",
        "    cands.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "    return cands\n",
        "\n",
        "def pick_diverse(ranked: List[Dict], k=6, max_per_source=1):\n",
        "    picked = []\n",
        "    per_src = {}\n",
        "    for d in ranked:\n",
        "        src = d[\"source\"]\n",
        "        per_src[src] = per_src.get(src, 0) + 1\n",
        "        if per_src[src] > max_per_source:\n",
        "            continue\n",
        "        picked.append(d)\n",
        "        if len(picked) >= k:\n",
        "            break\n",
        "    return picked\n"
      ],
      "metadata": {
        "id": "SQUdW9O9sb6o"
      },
      "id": "SQUdW9O9sb6o",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 13: Answer generation (FLAN-T5) + clean output + citations\n",
        "# What: generate short grounded answer using retrieved sources\n",
        "# Why: better UX, avoids hallucination, shows citations in video\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "LLM_NAME = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
        "llm = AutoModelForSeq2SeqLM.from_pretrained(LLM_NAME)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "llm = llm.to(device)\n",
        "\n",
        "def clean_answer(text: str, max_sentences: int = 4) -> str:\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    sents = re.split(r\"(?<=[.!?])\\s+\", text)\n",
        "    sents = [s.strip() for s in sents if s.strip()]\n",
        "    sents = sents[:max_sentences]\n",
        "    fixed = []\n",
        "    for s in sents:\n",
        "        fixed.append(s[:1].upper() + s[1:] if len(s) > 1 else s.upper())\n",
        "    out = \" \".join(fixed).strip()\n",
        "    if out and out[-1] not in \".!?\":\n",
        "        out += \".\"\n",
        "    return out\n",
        "\n",
        "def build_prompt(question: str, contexts: List[Dict]) -> str:\n",
        "    srcs = \"\\n\\n\".join([f\"[{i+1}] {c['text']}\" for i, c in enumerate(contexts)])\n",
        "    return (\n",
        "        \"You answer questions about food, diet, nutrition.\\n\"\n",
        "        \"Use ONLY the provided sources.\\n\"\n",
        "        \"Write 3-5 short sentences in simple language.\\n\"\n",
        "        \"If the sources are not enough, say: I don't have enough information in the indexed data.\\n\"\n",
        "        \"End with: Sources: [1], [2], ...\\n\\n\"\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        f\"Sources:\\n{srcs}\\n\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "def retrieve_dynamic(question: str, max_k=6, rerank_min=0.2, max_per_source=1,\n",
        "                     faiss_k=60, bm25_k=60, alpha=0.6, fused_limit=120):\n",
        "    dbg = prepare_query(question)\n",
        "    query_for_retrieval = dbg[\"expanded\"]\n",
        "\n",
        "    cands = retrieve_hybrid_candidates(query_for_retrieval, faiss_k=faiss_k, bm25_k=bm25_k, alpha=alpha)\n",
        "    ranked = rerank_top(dbg[\"rewritten\"], cands, limit=fused_limit)\n",
        "\n",
        "    # filter weak rerank\n",
        "    ranked = [d for d in ranked if d.get(\"rerank_score\", -1e9) >= rerank_min]\n",
        "\n",
        "    # diversify sources\n",
        "    ctx = pick_diverse(ranked, k=max_k, max_per_source=max_per_source)\n",
        "    return ctx, dbg\n",
        "\n",
        "def rag_answer(question: str, max_k=6, rerank_min=0.2, max_new_tokens=140, max_per_source=1):\n",
        "    ctxs, dbg = retrieve_dynamic(question, max_k=max_k, rerank_min=rerank_min, max_per_source=max_per_source)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"QUESTION:\", question)\n",
        "    print(\"üõ†Ô∏è Query debug:\")\n",
        "    for k, v in dbg.items():\n",
        "        print(f\" - {k:9s}: {v}\")\n",
        "\n",
        "    if not ctxs:\n",
        "        print(\"\\n‚úÖ ANSWER:\\nI don't have enough information in the indexed data.\")\n",
        "        return \"I don't have enough information in the indexed data.\", []\n",
        "\n",
        "    print(f\"\\nüìå SOURCES USED (K={len(ctxs)}):\")\n",
        "    for i, c in enumerate(ctxs, 1):\n",
        "        print(f\"{i}) {c['source']} | {c['chunk_id']} | rerank={c['rerank_score']:.3f}\")\n",
        "\n",
        "    prompt = build_prompt(question, ctxs)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = llm.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=4)\n",
        "\n",
        "    ans = tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
        "    ans = clean_answer(ans, max_sentences=4)\n",
        "\n",
        "    print(\"\\n‚úÖ ANSWER:\\n\", ans)\n",
        "    return ans, ctxs\n"
      ],
      "metadata": {
        "id": "iIZFFcZOseE2"
      },
      "id": "iIZFFcZOseE2",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 14: Demo (your problematic typo query included)\n",
        "# What: quick test to confirm pipeline is stable\n",
        "# Why: validates spelling + retrieval + generation end-to-end\n",
        "# ============================================================\n",
        "\n",
        "_ = rag_answer(\"Why is junk food considered unhealthy?\")\n",
        "_ = rag_answer(\"What are calories and how are they measured?\")\n",
        "_ = rag_answer(\"Wht is healty to eat?\")\n",
        "_ = rag_answer(\"difference between macronutrients and micronutrients\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHni8kGtsfje",
        "outputId": "9cefd656-6da4-4415-bf14-1e9ee9e57ef7"
      },
      "id": "cHni8kGtsfje",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "QUESTION: Why is junk food considered unhealthy?\n",
            "üõ†Ô∏è Query debug:\n",
            " - original : Why is junk food considered unhealthy?\n",
            " - typo_fixed: why is junk food considered unhealthy\n",
            " - rewritten: why is junk food considered unhealthy?\n",
            " - expanded : why is junk food considered unhealthy debris nutrient see insalubrious processed ultra processed high sugar high salt high fat fast food snacks sugary drinks\n",
            "\n",
            "üìå SOURCES USED (K=2):\n",
            "1) JunkFood.pdf | JunkFood.pdf::chunk10 | rerank=5.656\n",
            "2) Junk food and your health _ healthdirect.html | Junk food and your health _ healthdirect.html::chunk3 | rerank=3.807\n",
            "\n",
            "‚úÖ ANSWER:\n",
            " Junk foods are mainly made up by using a lot of saturated fats which are unhealthy after digestion and release a lot of toxins into the body. Moreover, it lacks vitamins and minerals which are necessary to have good health and immunity to fight diseases. The practice of high consumption of junk foods like magi noodles, burgers, sandwiches, hot dogs, patties, pastries, popcorn, potato chips, carbonated drinks, biscuits, muffins, toast, chocolates etc. Have become common feature of adolescent‚Äôs diet.\n",
            "\n",
            "======================================================================\n",
            "QUESTION: What are calories and how are they measured?\n",
            "üõ†Ô∏è Query debug:\n",
            " - original : What are calories and how are they measured?\n",
            " - typo_fixed: what are calorie and how are they measured\n",
            " - rewritten: what are calorie and how are they measured?\n",
            " - expanded : what are calorie and how are they measured kilocalorie measure\n",
            "\n",
            "üìå SOURCES USED (K=3):\n",
            "1) Nutrition.txt | Nutrition.txt::chunk12 | rerank=2.488\n",
            "2) Food_energy.txt | Food_energy.txt::chunk1 | rerank=2.387\n",
            "3) Dietary_Guidelines_for_Americans_2020-2025.pdf | Dietary_Guidelines_for_Americans_2020-2025.pdf::chunk360 | rerank=0.740\n",
            "\n",
            "‚úÖ ANSWER:\n",
            " The energy provided by macronutrients in food is measured in kilocalories, usually called Calories, where 1 Calorie is the amount of energy required to raise 1 kilogram of water by 1 degree Celsius. Carbohydrates are molecules that store significant amounts of energy. Animals digest and metabolize carbohydrates to obtain this energy. They include sugars, oligosaccharides, and polysaccharides.\n",
            "\n",
            "======================================================================\n",
            "QUESTION: Wht is healty to eat?\n",
            "üõ†Ô∏è Query debug:\n",
            " - original : Wht is healty to eat?\n",
            " - typo_fixed: what is healthy to eat\n",
            " - rewritten: what is healthy to eat?\n",
            " - expanded : what is healthy to eat salubrious balanced nutrient dense whole foods vegetables fruits fiber\n",
            "\n",
            "üìå SOURCES USED (K=4):\n",
            "1) Dietary_Guidelines_for_Americans_2020-2025.pdf | Dietary_Guidelines_for_Americans_2020-2025.pdf::chunk97 | rerank=3.236\n",
            "2) Dietary_Guidelines_for_Americans.txt | Dietary_Guidelines_for_Americans.txt::chunk13 | rerank=3.193\n",
            "3) Eating a balanced diet - NHS.html | Eating a balanced diet - NHS.html::chunk1 | rerank=2.664\n",
            "4) Junk food and your health _ healthdirect.html | Junk food and your health _ healthdirect.html::chunk10 | rerank=1.284\n",
            "\n",
            "‚úÖ ANSWER:\n",
            " A healthy eating pattern is important at every stage of life and can have positive effects that add up over time. It‚Äôs important to eat a variety of fruits, vegetables, grains, dairy or fortified soy alternatives, and protein foods. When deciding what to eat or drink, choose options that are full of nutrients. Make every bite count.\n",
            "\n",
            "======================================================================\n",
            "QUESTION: difference between macronutrients and micronutrients\n",
            "üõ†Ô∏è Query debug:\n",
            " - original : difference between macronutrients and micronutrients\n",
            " - typo_fixed: difference between macronutrients and micronutrient\n",
            " - rewritten: difference between macronutrients and micronutrient?\n",
            " - expanded : difference between macronutrients and micronutrient deviation betwixt\n",
            "\n",
            "üìå SOURCES USED (K=4):\n",
            "1) Micronutrient.txt | Micronutrient.txt::chunk0 | rerank=7.141\n",
            "2) Human_nutrition.txt | Human_nutrition.txt::chunk5 | rerank=6.462\n",
            "3) Nutrition.txt | Nutrition.txt::chunk3 | rerank=5.908\n",
            "4) Nutrient.txt | Nutrient.txt::chunk2 | rerank=3.916\n",
            "\n",
            "‚úÖ ANSWER:\n",
            " Micronutrients are essential chemicals required by organisms in small quantities to perform various biogeochemical processes and regulate physiological functions of cells and organs. By enabling these processes, micronutrients support the health of organisms throughout life. For humans, micronutrients typically take one of three forms: vitamins, trace elements, and dietary minerals. Human micronutrient requirements are in amounts generally less than 100 milligrams per day, whereas macronutrients are required in gram quantities daily.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 15: Tiny Evaluation (Recall@K on a mini set)\n",
        "# What: simple offline check of retrieval quality\n",
        "# ============================================================\n",
        "\n",
        "# Define a small gold set manually (for demo)\n",
        "# Each query maps to keywords that should appear in at least one retrieved chunk.\n",
        "EVAL_QUERIES = [\n",
        "    (\"What are calories?\", [\"kilocalorie\", \"kcal\", \"food energy\", \"calorie\"]),\n",
        "    (\"Why is junk food unhealthy?\", [\"high sugar\", \"high salt\", \"high fat\", \"ultra-processed\", \"low nutrients\"]),\n",
        "    (\"What is a balanced diet?\", [\"balanced\", \"nutrient\", \"vegetables\", \"fruits\", \"whole\"]),\n",
        "]\n",
        "\n",
        "def recall_at_k(question: str, must_contain: List[str], k=5):\n",
        "    ctxs, dbg = retrieve_dynamic(question, max_k=k, rerank_min=0.0)\n",
        "    joined = \" \".join([c[\"text\"].lower() for c in ctxs])\n",
        "    hits = sum(1 for kw in must_contain if kw.lower() in joined)\n",
        "    return hits / max(1, len(must_contain)), ctxs\n",
        "\n",
        "for q, kws in EVAL_QUERIES:\n",
        "    r, ctxs = recall_at_k(q, kws, k=5)\n",
        "    print(\"\\nQ:\", q)\n",
        "    print(\"Recall@5:\", round(r, 3), \"| hits:\", int(r*len(kws)), \"/\", len(kws))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjmDdWA9sgTl",
        "outputId": "63d3b178-4e18-4d0e-a28b-78bce0a89098"
      },
      "id": "gjmDdWA9sgTl",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What are calories?\n",
            "Recall@5: 1.0 | hits: 4 / 4\n",
            "\n",
            "Q: Why is junk food unhealthy?\n",
            "Recall@5: 0.2 | hits: 1 / 5\n",
            "\n",
            "Q: What is a balanced diet?\n",
            "Recall@5: 0.4 | hits: 2 / 5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0280f9631aaa48af97f21d0770703043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9b6595383c743848414044b0ed6b4b8",
              "IPY_MODEL_57b7cb94f85a4bfab5aaeda7af3193bf",
              "IPY_MODEL_d8b8046e415a4bcd994ee22aa53f409a"
            ],
            "layout": "IPY_MODEL_5e25e8bbcf4c44ea9213c08689f47f0f"
          }
        },
        "e9b6595383c743848414044b0ed6b4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_523249dc955a4f2a90673005bb1ead0c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e81ae0d053954deaabbc023225936e4a",
            "value": "Batches:‚Äá100%"
          }
        },
        "57b7cb94f85a4bfab5aaeda7af3193bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_240c291eeb4245b7b61e0fc309c4b9d6",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39d82cf6bc734d8796a9b77f45663781",
            "value": 26
          }
        },
        "d8b8046e415a4bcd994ee22aa53f409a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a949d669e68e4e98bc7cfe61adde0b37",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fbfb0a2f40914ef2a6a7c3caf634cb3a",
            "value": "‚Äá26/26‚Äá[00:06&lt;00:00,‚Äá‚Äá3.08it/s]"
          }
        },
        "5e25e8bbcf4c44ea9213c08689f47f0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "523249dc955a4f2a90673005bb1ead0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e81ae0d053954deaabbc023225936e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "240c291eeb4245b7b61e0fc309c4b9d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d82cf6bc734d8796a9b77f45663781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a949d669e68e4e98bc7cfe61adde0b37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbfb0a2f40914ef2a6a7c3caf634cb3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}