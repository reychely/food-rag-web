{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T11:08:43.015234Z",
     "start_time": "2026-01-13T11:08:42.997922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install packages for multi-format loading + indexing\n",
    "# - Checks which packages are already installed\n",
    "# - Installs only missing ones\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "REQUIRED_PACKAGES = {\n",
    "    \"sentence-transformers\": \"sentence_transformers\",\n",
    "    \"faiss-cpu\": \"faiss\",\n",
    "    \"transformers\": \"transformers\",\n",
    "    \"accelerate\": \"accelerate\",\n",
    "    \"pypdf\": \"pypdf\",\n",
    "    \"python-docx\": \"docx\",\n",
    "    \"beautifulsoup4\": \"bs4\",\n",
    "    \"rank-bm25\": \"rank_bm25\",\n",
    "    \"tqdm\": \"tqdm\",\n",
    "    \"streamlit\": \"streamlit\",\n",
    "    \"requests\": \"requests\"\n",
    "}\n",
    "\n",
    "def is_installed(module_name: str) -> bool:\n",
    "    return importlib.util.find_spec(module_name) is not None\n",
    "\n",
    "missing = []\n",
    "for pip_name, module_name in REQUIRED_PACKAGES.items():\n",
    "    if not is_installed(module_name):\n",
    "        missing.append(pip_name)\n",
    "\n",
    "if missing:\n",
    "    print(\"ðŸ“¦ Installing missing packages:\", missing)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *missing])\n",
    "    print(\"âœ… Installation complete\")\n",
    "else:\n",
    "    print(\"âœ… All required packages already installed\")\n",
    "\n"
   ],
   "id": "84ab8ffd0d868803",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All required packages already installed\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T11:08:43.099512Z",
     "start_time": "2026-01-13T11:08:43.092230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define project folders\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Running in Google Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE = Path(\"/content/drive/MyDrive/IR_RAG_App\")\n",
    "    print(\"âœ… Running in Google Colab\")\n",
    "else:\n",
    "    # Running locally (Jupyter / VS Code)\n",
    "    BASE = Path(\"./IR_RAG_App\")\n",
    "    print(\"âœ… Running locally\")\n",
    "\n",
    "DATA_DIR    = BASE / \"data_raw\"\n",
    "STORAGE_DIR = BASE / \"storage\"\n",
    "APP_DIR     = BASE / \"webapp\"\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STORAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "APP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"BASE:\", BASE.resolve())\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve())\n",
    "print(\"STORAGE_DIR:\", STORAGE_DIR.resolve())\n",
    "print(\"APP_DIR:\", APP_DIR.resolve())\n"
   ],
   "id": "69f6171baef5052a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Running locally\n",
      "BASE: C:\\Users\\reychel\\Documents\\GitHub\\food-rag-web\\IR_RAG_App\n",
      "DATA_DIR: C:\\Users\\reychel\\Documents\\GitHub\\food-rag-web\\IR_RAG_App\\data_raw\n",
      "STORAGE_DIR: C:\\Users\\reychel\\Documents\\GitHub\\food-rag-web\\IR_RAG_App\\storage\n",
      "APP_DIR: C:\\Users\\reychel\\Documents\\GitHub\\food-rag-web\\IR_RAG_App\\webapp\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T11:08:43.773225Z",
     "start_time": "2026-01-13T11:08:43.108142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load documents from DATA_DIR (supports txt/pdf/docx/html)\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import docx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "SUPPORTED_EXT = {\".txt\", \".md\", \".pdf\", \".docx\", \".html\", \".htm\"}\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace(\"\\x00\", \" \")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_txt_file(path: Path) -> str:\n",
    "    return clean_text(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "def load_pdf_file(path: Path) -> str:\n",
    "    reader = PdfReader(str(path))\n",
    "    pages = [(p.extract_text() or \"\") for p in reader.pages]\n",
    "    return clean_text(\"\\n\".join(pages))\n",
    "\n",
    "def load_docx_file(path: Path) -> str:\n",
    "    d = docx.Document(str(path))\n",
    "    return clean_text(\"\\n\".join(p.text for p in d.paragraphs))\n",
    "\n",
    "def load_html_file(path: Path) -> str:\n",
    "    html = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "        tag.decompose()\n",
    "    return clean_text(soup.get_text(\"\\n\"))\n",
    "\n",
    "def load_one_file(path: Path) -> str:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in {\".txt\", \".md\"}:\n",
    "        return load_txt_file(path)\n",
    "    if ext == \".pdf\":\n",
    "        return load_pdf_file(path)\n",
    "    if ext == \".docx\":\n",
    "        return load_docx_file(path)\n",
    "    if ext in {\".html\", \".htm\"}:\n",
    "        return load_html_file(path)\n",
    "    return \"\"\n",
    "\n",
    "def load_documents(data_dir: Path) -> List[Dict]:\n",
    "    docs = []\n",
    "    for p in sorted(data_dir.glob(\"**/*\")):\n",
    "        if p.is_dir():\n",
    "            continue\n",
    "        if p.suffix.lower() not in SUPPORTED_EXT:\n",
    "            continue\n",
    "\n",
    "        text = load_one_file(p)\n",
    "        if len(text) < 400:   # filter tiny/noisy docs\n",
    "            continue\n",
    "\n",
    "        docs.append({\n",
    "            \"source\": p.name,\n",
    "            \"path\": str(p),\n",
    "            \"ext\": p.suffix.lower(),\n",
    "            \"text\": text\n",
    "        })\n",
    "    return docs\n",
    "\n",
    "# Quick preview\n",
    "docs = load_documents(DATA_DIR)\n",
    "print(f\"âœ… Loaded {len(docs)} documents from: {DATA_DIR}\")\n",
    "if docs:\n",
    "    print(\"Example:\", docs[0][\"source\"], \"| ext:\", docs[0][\"ext\"], \"| chars:\", len(docs[0][\"text\"]))\n"
   ],
   "id": "87880ebb7e5026",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 0 documents from: IR_RAG_App\\data_raw\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T11:08:44.332845Z",
     "start_time": "2026-01-13T11:08:43.797298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chunk the documents (chunking improves retrieval + generation)\n",
    "\n",
    "CHUNK_SIZE = 900\n",
    "CHUNK_OVERLAP = 150\n",
    "\n",
    "def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    chunks = []\n",
    "    n = len(text)\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        c = text[start:end].strip()\n",
    "        if c:\n",
    "            chunks.append(c)\n",
    "        start += step\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for d in docs:\n",
    "    parts = chunk_text(d[\"text\"])\n",
    "    for i, c in enumerate(parts):\n",
    "        chunks.append({\n",
    "            \"source\": d[\"source\"],\n",
    "            \"path\": d[\"path\"],\n",
    "            \"ext\": d[\"ext\"],\n",
    "            \"chunk_id\": f\"{d['source']}::chunk{i}\",\n",
    "            \"text\": c\n",
    "        })\n",
    "\n",
    "print(f\"âœ… Created {len(chunks)} chunks from {len(docs)} documents\")\n",
    "print(\"Preview:\", chunks[0][\"chunk_id\"], \"|\", chunks[0][\"text\"][:180], \"...\")\n"
   ],
   "id": "5bb4fa05ebfb1a8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 0 chunks from 0 documents\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 32\u001B[0m\n\u001B[0;32m     23\u001B[0m         chunks\u001B[38;5;241m.\u001B[39mappend({\n\u001B[0;32m     24\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m: d[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     25\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m: d[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     28\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: c\n\u001B[0;32m     29\u001B[0m         })\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mâœ… Created \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(chunks)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m chunks from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(docs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m documents\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPreview:\u001B[39m\u001B[38;5;124m\"\u001B[39m, chunks[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchunk_id\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m\"\u001B[39m, chunks[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m][:\u001B[38;5;241m180\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build embeddings + FAISS index and save to STORAGE_DIR (persistent)\n",
    "# Re-run this block whenever you add new files.\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "INDEX_PATH = STORAGE_DIR / \"faiss.index\"\n",
    "META_PATH  = STORAGE_DIR / \"chunks_meta.json\"\n",
    "EMB_PATH   = STORAGE_DIR / \"chunk_embs.npy\"  # saved embeddings for MMR/rerank later\n",
    "\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "X = embedder.encode(\n",
    "    texts,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True\n",
    ").astype(\"float32\")\n",
    "\n",
    "index = faiss.IndexFlatIP(X.shape[1])  # cosine similarity using normalized vectors\n",
    "index.add(X)\n",
    "\n",
    "faiss.write_index(index, str(INDEX_PATH))\n",
    "META_PATH.write_text(json.dumps(chunks, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "np.save(EMB_PATH, X)\n",
    "\n",
    "print(\"âœ… Embeddings shape:\", X.shape)\n",
    "print(\"âœ… FAISS index size:\", index.ntotal)\n",
    "print(\"âœ… Saved index:\", INDEX_PATH)\n",
    "print(\"âœ… Saved metadata:\", META_PATH)\n",
    "print(\"âœ… Saved embeddings:\", EMB_PATH)\n"
   ],
   "id": "5bb98a074395c678"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Retrieval = Hybrid (BM25 + FAISS) + Rerank\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "import re, json, faiss\n",
    "\n",
    "# load persistent artifacts\n",
    "index = faiss.read_index(str(INDEX_PATH))\n",
    "chunks_meta = json.loads(META_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    return [t for t in text.split() if len(t) > 2]\n",
    "\n",
    "# BM25 index\n",
    "bm25 = BM25Okapi([tokenize(c[\"text\"]) for c in chunks_meta])\n",
    "\n",
    "# Reranker model\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def retrieve_hybrid(query, faiss_k=30, bm25_k=30, alpha=0.6):\n",
    "    qv = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    faiss_scores, faiss_ids = index.search(qv, faiss_k)\n",
    "    faiss_scores, faiss_ids = faiss_scores[0], faiss_ids[0]\n",
    "\n",
    "    q_tokens = tokenize(query)\n",
    "    bm25_scores = bm25.get_scores(q_tokens)\n",
    "    bm25_top = np.argsort(bm25_scores)[::-1][:bm25_k]\n",
    "\n",
    "    # normalize for merge\n",
    "    faiss_s = (faiss_scores - faiss_scores.min()) / (faiss_scores.max() - faiss_scores.min() + 1e-9)\n",
    "    bm25_s = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-9)\n",
    "\n",
    "    cand = set(map(int, faiss_ids.tolist())) | set(map(int, bm25_top.tolist()))\n",
    "    faiss_map = {int(i): float(s) for i, s in zip(faiss_ids, faiss_s)}\n",
    "\n",
    "    merged = []\n",
    "    for cid in cand:\n",
    "        score = alpha * faiss_map.get(cid, 0.0) + (1 - alpha) * float(bm25_s[cid])\n",
    "        item = chunks_meta[cid]\n",
    "        merged.append({**item, \"score\": float(score)})\n",
    "\n",
    "    merged.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return merged\n",
    "\n",
    "def rerank(query, docs, top_n=5):\n",
    "    pairs = [(query, d[\"text\"]) for d in docs[:50]]  # rerank only top-50 for speed\n",
    "    scores = reranker.predict(pairs)\n",
    "    for d, s in zip(docs[:50], scores):\n",
    "        d[\"rerank_score\"] = float(s)\n",
    "    docs[:50] = sorted(docs[:50], key=lambda x: x.get(\"rerank_score\", -1e9), reverse=True)\n",
    "    return docs[:top_n]\n",
    "\n",
    "def retrieve_best(query, k=5):\n",
    "    cands = retrieve_hybrid(query)\n",
    "    top = rerank(query, cands, top_n=k)\n",
    "    return top\n",
    "\n",
    "# quick test\n",
    "test = retrieve_best(\"What are calories and how are they measured?\", k=4)\n",
    "[(t[\"source\"], round(t[\"score\"],3), round(t[\"rerank_score\"],3)) for t in test]\n"
   ],
   "id": "8a762fde29970340"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Final RAG answering (short answers + source citations)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "LLM_NAME = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
    "llm = AutoModelForSeq2SeqLM.from_pretrained(LLM_NAME)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "llm = llm.to(device)\n",
    "\n",
    "def build_prompt(question, contexts):\n",
    "    sources_block = \"\\n\\n\".join([f\"[{i+1}] ({c['source']} | {c['chunk_id']})\\n{c['text']}\"\n",
    "                                 for i, c in enumerate(contexts)])\n",
    "    return (\n",
    "        \"Answer clearly in 3â€“5 short sentences.\\n\"\n",
    "        \"Use simple language.\\n\"\n",
    "        \"Use ONLY the sources.\\n\"\n",
    "        \"If not enough info, say: I don't have enough information in the indexed data.\\n\"\n",
    "        \"End with: Sources: [1], [2], ... (only those you used)\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Sources:\\n{sources_block}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "def rag_answer(question, k=5, max_new_tokens=170):\n",
    "    contexts = retrieve_best(question, k=k)\n",
    "    prompt = build_prompt(question, contexts)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = llm.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=4)\n",
    "    answer = tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    return answer, contexts\n",
    "\n",
    "# demo\n",
    "q = \"Why is junk food considered unhealthy?\"\n",
    "ans, ctx = rag_answer(q, k=4)\n",
    "print(ans)\n",
    "print(\"\\nTop sources:\", [c[\"source\"] for c in ctx])"
   ],
   "id": "eec848ea7dc47845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "71d63fd6ee054c38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9d84f90710de578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1141f2cdb540cd20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "df9860e69c7ce04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "616785208f174e72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T11:10:49.683156Z",
     "start_time": "2026-01-13T11:10:49.672404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3B: Copy files from another folder into DATA_DIR\n",
    "# Change SOURCE_FOLDER to where your files currently are.\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "SOURCE_FOLDER = Path(r\"C:\\path\\to\\your\\files\")  # <-- change this\n",
    "\n",
    "allowed = {\".txt\",\".md\",\".pdf\",\".docx\",\".html\",\".htm\"}\n",
    "\n",
    "copied = 0\n",
    "for p in SOURCE_FOLDER.glob(\"**/*\"):\n",
    "    if p.is_file() and p.suffix.lower() in allowed:\n",
    "        shutil.copy2(p, DATA_DIR / p.name)\n",
    "        copied += 1\n",
    "\n",
    "print(\"âœ… Copied files:\", copied)\n",
    "print(\"Now DATA_DIR contains:\", len(list(DATA_DIR.glob('*'))), \"items\")\n"
   ],
   "id": "b5bc9e8e70a6d8cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Copied files: 0\n",
      "Now DATA_DIR contains: 0 items\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
